---
title: "Excercise 2"
subtitle: "TMA4300 Computer intensive statistical methods Spring 2021"
author: 
- "Johan Fredrik Agerup"
- "Arne Rustad"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
header-includes:
   - \usepackage{subfig}
---

\newcommand{\vect}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\matr}[1]{\ensuremath{\boldsymbol{\mathbf{#1} }}}
\newcommand{\Var}{\textrm{Var}}
\newcommand{\E}{\textrm{E}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\Corr}{\textrm{Corr}}

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```


```{r rpackages,eval=TRUE,echo=FALSE}
library(ggplot2)
# library(expm)
# library(Matrix)
library(dplyr)
library(tidyverse)
library(boot)
library(stats)
library(latex2exp)
library(coda)
library(INLA)
library(mvtnorm)
library(MASS)
```

# Problem A

In this problem we analyze a data set of time intervals between successive coal-mining disaster in the UK involving ten or more men killed. The data is for the period March 15th 1851 to March 22nd 1962. Although the description of the dataset says that all 191 dates in the dataset are dates of explosions, to be in compliance with the exercise text, the first and last records are assumed to be start and end dates. This results in 189 observations in the time period.

## 1)

First we try to get an impression of the data set by making a plot with year along the $x$-axis and the cumulative number of disasters along the $y$-axis.


```{r, fig.width = 6, fig.cap = "\\label{fig:1a_cumplot} Cumulative number of explosions which resulted in 10 or more fatalities. The time span is from March 15 1851 until March 22 1962."}
df.coal = coal # Get the data into a new data frame variable


# add cumulative number of explosions at each time, letting first and last date be respectively start and end date of study
df.coal$cum.n.explosions = c(1, cumsum(rep(1, nrow(coal)-2)), nrow(coal)-2)

# plot the cumulative number of explosions as a function of time
ggplot(df.coal, aes(x = date, y = cum.n.explosions)) + geom_line() +
  
  ggtitle("Cumulative number of explosions (15.03.1851-22.3.1962)") +
  
  xlab("Year") + ylab("Cumlative number of explosions") +
  
  geom_vline(aes(xintercept = 1890, linetype = "Year 1890")) +
  
  geom_vline((aes(xintercept = 1945, linetype = "Year 1945"))) +
  
  guides(linetype = guide_legend("Vertical Lines"))
```

From Figure \ref{fig:1a_cumplot} the rate of accidents appear approximately constant from year 1850 until around 1890. Then the rate of large explosions appear to dampen a bit, perhaps due to better safety rutines, better equipment, change in societal norms and laws, less demand for coal or another reason. There also appears to be a significant change in rate of accidents around the end of world war 2, i.e year 1945. If we were trying to estimate two break points for change in rate of accidents, then these two might be quite good options. However, if we only wanted to estimate one it would probably be around 1890.


## 2)

To analyze the data set we adopt a hierarchical Bayesian model. Assume the coal-mining disasters to follow an inhomogeneous Poisson process with intensity function $\lambda(t)$ (number of events per year). Assume $\lambda(t)$ to be piecewise constant with $n$ breakpoints. Let $t_0$ and $t_{n+1}$ denote the start and end times for the dataset and let $t_k; \; k=1,2,\dots, n$ denote the break points of the intensity function. Thus,

$$ \lambda(t) =
\begin{cases}
\lambda_{k-1} \; &\textrm{for } t \in [t_{k-1}, t_k] \\
\lambda_n \; &\textrm{for } t \in [t_n, t_{n+1}]
\end{cases}$$

Thereby the parameters of the model is $t_1, \dots, t_n$ and $\lambda_0, \dots, \lambda_n$ where $t_0 < t_1 < \dots < t_n < t_{n+1}$. By subdividing the observations into short intervals and taking the limit when the length of these intervals go to zero, the likelihood function for the observed data can be derived as


\begin{align*}
f (x | t_1, \dots, t_n, \lambda_0, \dots, \lambda_n)
&= e^{-\int_{t_0}^{t_{n+1}} \lambda(t) dt} \prod_{k=0}^{n} \lambda_k^{y_k} \\
&= e^{-\sum_{k = 0}^{n} \lambda_k (t_{k+1} - t_k)} \prod_{k=0}^{n} \lambda_k^{y_k}.
\end{align*}
a
Here $\vect x$ is the observed data and $y_k$ is the number of observed disasters in the period $t_k$ to $t_{k+1}$. Assume $t_1, \dots, t_n$ to be apriori uniformly distributed on the allowed values and $\lambda_0, \dots, \lambda_n$ to be apriori independent of $t_1, \dots, t_n$ and apriori independent of each other. Apriori we assume all $\lambda_0, \dots, \lambda_n$ to be distributed from the same gamma distribution with shape parameter $\alpha = 2$ and scale parameter $\beta$, i.e

\begin{align*}
\pi (\lambda_i | \beta) = \frac{1}{\beta^2} \lambda_i e^{\frac{-\lambda_i}{\beta}} \; \textrm{for } \lambda_i \geq 0
\end{align*}

Finally, for $\beta$ we use the improper prior

$$ \pi(\beta) \propto \frac{e^{\frac{-1}{\beta}}}{\beta} \; \textrm{for } \beta > 0$$

In the following it is assumed $n = 1$, resulting in $\vect{\theta} = (t_1, \lambda_0, \lambda_1, \beta)$.

The posterior distribution is then

\begin{align*}
\pi(\vect{\theta} | \vect x) &= f(\vect x | \vect{\theta}) \pi(\vect{\theta}) \\
&= f(\vect x | t_1,  \lambda_0, \lambda_1) \pi(t_1) \pi(\lambda_0 | \beta) \pi(\lambda_1 | \vect{\theta}) \pi(\beta)
\end{align*}

Here it is used that $t_1, \lambda_0$ and $\lambda_1$ all are independent of each other. Inserting the expressions for the likelihood and the priors we get an expression for the posterior distribution up to a proportionality constant

\begin{align*}
\pi(\vect{\theta} = (t_1, \lambda_0, \lambda_1, \beta) | \vect x)
&\propto e^{-\lambda_0 (t_{1} - t_0) - \lambda_1 (t_{2} - t_1)} \lambda_0^{y_0} \lambda_1^{y_1}
\frac{1}{t_{2} - t_0}
\frac{1}{\beta^2} \lambda_0 e^{\frac{-\lambda_0}{\beta}}
\frac{1}{\beta^2} \lambda_1 e^{\frac{-\lambda_1}{\beta}}
\frac{e^{\frac{-1}{\beta}}}{\beta} \\
&\propto e^{-\lambda_0 (t_{1} - t_0) - \lambda_1 (t_{2} - t_1) - \frac{\lambda_0 + \lambda_1 + 1}{\beta}}
\lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1} \beta^{-5}
\end{align*}

## 3)

Next we want to find the full conditionals of $\vect{\theta}$ for later use in the implementation of MCMC algorithms to find the posterior. Let $\vect{\theta}^{-j}$ denote all of vector $\vect{\theta}$ except for the jth element, i.e $\vect{\theta}^{-j} = [\theta^1, \dots, \theta^{j-1}, \theta^{j+1}, \dots, \theta^4]$. The full conditional of element $j$ in component vector $\vect{\theta}$ is defined as

$$ \pi (\theta^j | \vect{\theta}^{-j}, \vect x) = \frac{\pi(\vect{\theta} | \vect x)}{\pi (\vect{\theta}^{-j} | \vect x)} \propto  \pi(\vect{\theta} | \vect x).$$

Thus, the non-normalised conditional densities of $\theta^j | \vect{\theta}^{-j}$ can be directly derived from $\pi(\vect{\theta} | \vect x)$ by omitting all multiplicative factors that do not depend on $\theta_j$.

The full conditional of $t_1$ is

$$ \pi(t_1 | \vect x, \lambda_0, \lambda_1, \beta) \propto e^{t_1 (\lambda_1 - \lambda_0)} \lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1}, \qquad t_1 \in [t_0, t_2].$$

The full conditional of $t_1$ is not recognized as a known distribution.

The full conditional of $\lambda_0$ is

\begin{equation} \label{eq:fc_lambda0}
\pi(\lambda_0 | \vect{x}, t_1, \lambda_1, \beta) \propto \lambda_0^{y_0 + 1} e^{-\lambda_0(t_1 - t_0 + \frac{1}{\beta})}, \qquad \lambda_0 \geq 0.
\end{equation}


We recognize the full conditional of $\lambda_0$ as the Gamma$(y_0 + 2, \frac{1}{t_1 - t_0 + \frac{1}{\beta}})$ distribution. Similarily, the full conditional of $\lambda_1$ is

\begin{equation} \label{eq:fc_lambda1}
\pi(\lambda_1 | \vect{x}, t_1, \lambda_0, \beta) \propto \lambda_1^{y_1 + 1} e^{-\lambda_1(t_2 - t_1 + \frac{1}{\beta})}, \qquad \lambda_1 \geq 0.
\end{equation}

From this we see that the full conditional of $\lambda_1$ is Gamma$(y_1 + 2, \frac{1}{t_2 - t_1 + \frac{1}{\beta}})$ distributed. Lastly, the expression for the full conditional of $\beta$ is

\begin{equation} \label{eq:fc_beta}
\pi(\beta | \vect{x}, t_1, \lambda_0, \lambda_1) \propto  \frac{1}{\beta^5} e^{\frac{-(1 + \lambda_0 + \lambda_1)}{\beta}}, \qquad \beta >0.
\end{equation}

Here $\beta$ is $\textrm{Inverse Gamma}(4, 1 + \lambda_0 + \lambda_1)$ distributed where the first parameter is the shape parameter and the second is the scale parameter. 




## 4)

In this task we want to implement a single site MCMC algorithm for the posterior distribution $\pi(\vect{\theta} | \vect{x})$. Since three out of four full conditionals are known distributions we implement a Hybrid Gibbs sampler (Metropolis-within-Gibbs). This way three out of four components can be sampled very efficiently.

Before we proceed we want to determine some notation. Let $\vect \theta_i$ be ith component vector in the MCMC algorithm and let $Q(\tilde \theta ^j | \theta_{i-1}^j, \vect \theta_{i-1}^{-j})$ be the proposal distribution for component j at the ith iteration where $\tilde \theta^j$ is the proposal. Here $\vect \theta_{i-1}^{-j} = [\theta_{i}^1, \dots, \theta_{i}^{j-1}, \theta_{i-1}^{j+1}, \dots, \theta_{i-1}^4]$ in accordance with the definition in task A3. The corresponding acceptance probability of the proposal for component $j$ at iteration $i$ is denoted $\alpha(\tilde \theta^j | \vect \theta_{i-1}^j,  \theta_{i-1}^{-j})$ and can be expressed as


$$\alpha(\tilde \theta^j | \theta_{i-1}^j,  \vect \theta_{i-1}^{-j})
= \min \left \{
1, 
\frac{\pi (\tilde \theta^j |  \vect \theta_{i-1}^{-j})}{\pi (\theta_{i-1}^j |  \vect \theta_{i-1}^{-j})} 
\frac{Q (\theta_{i-1}^j | \tilde \theta^j,  \vect \theta_{i-1}^{-j})}{Q (\tilde \theta^j |  \theta_{i-1}^j,  \vect \theta_{i-1}^{-j})} 
\right \}.$$

Note that for readability $\vect x$ is excluded from the given parameter list for the posterior distribution $\pi$, proposal distribution $Q$ and acceptance probability $\alpha$ expressions. This is done for the rest of task A.

For $\lambda_0, \lambda_1$ and $\beta$ the proposal distributions for iteration i are given by respectively equations (\ref{eq:fc_lambda0}), (\ref{eq:fc_lambda1}) and (\ref{eq:fc_beta}).

\begin{align}
\label{eq:fc_lambda0_np}
Q(\tilde \theta_{i}^{\lambda_0} | \theta_{i-1}^{\lambda_0}, \vect \theta_{i-1}^{- \tilde \lambda_0})& = \pi (\tilde \theta_{i}^{\lambda_0} | \vect \theta_{i-1}^{- \lambda_0}) 
= \frac{(t_1 - t_0 + \frac{1}{\beta})^{y_0+2}}{\Gamma(y_0+2)}
\tilde \lambda_0^{y_0 + 1} e^{-\lambda_0(t_1 - t_0 + \frac{1}{\beta})} 0\\
\label{eq:fc_lambda1_np}
Q(\tilde \theta_{i}^{\lambda_1} | \theta_{i-1}^{\lambda_1}, \vect \theta_{i-1}^{- \tilde \lambda_1})& = \pi (\tilde \theta_{i}^{\lambda_1} | \vect \theta_{i-1}^{- \lambda_1}) 
= \frac{(t_2 - t_1 + \frac{1}{\beta})^{y_1+2}}{\Gamma(y_1+2)}
\tilde \lambda_1^{y_1 + 1} e^{-\lambda_1(t_2 - t_1 + \frac{1}{\beta})} \\
\label{eq:fc_beta_np}
Q(\tilde \theta_{i}^{\beta} | \theta_{i-1}^{\beta}, \vect \theta_{i-1}^{- \beta})& = \pi (\tilde \theta_{i}^{\beta} | \vect \theta_{i-1}^{- \beta})
= \frac{(1 + \lambda_0 + \lambda_1)^6}{\Gamma(6)} \frac{1}{\tilde \beta^5} e^{\frac{-(1 + \lambda_0 + \lambda_1)}
{\tilde \beta}}
\end{align}

Note that for readability the iteration index is excluded from $t_1, \lambda_0, \lambda_1$ and $\beta$ when writing out the full conditionals. This will also be done for other instances where including the iteration indexes would hinder readability drastically. Since all three parameters are updated iteratively from the corresponding univariate full conditionals the acceptance probabilities are always exactly equal to 1, since 
$\alpha(\tilde \theta^j | \theta_{i-1}^j,  \vect \theta_{i-1}^{-j})
= \min \left \{
1, \frac{\pi (\tilde \theta^j |  \vect \theta_{i-1}^{-j})}{\pi (\theta_{i-1}^j |  \vect \theta_{i-1}^{-j})} 
\frac{\pi (\theta_{i-1}^j |  \vect \theta_{i-1}^{-j})}{\pi (\tilde \theta^j |  \vect \theta_{i-1}^{-j})}
\right \}
= \min \left \{ 1, 1 \right \}
= 1$. 
That is

\begin{align}
\label{eq:accept_beta}
\alpha(\tilde \theta_{i}^{\lambda_0} | \theta_{i-1}^{\lambda_0}, \vect \theta_{i-1}^{- \lambda_0})& = 1 \\
\label{eq:accept_lambda0}
\alpha(\tilde \theta_{i}^{\lambda_1} | \theta_{i-1}^{\lambda_1}, \vect \theta_{i-1}^{- \lambda_1})& = 1 \\
\label{eq:accept_lambda1}
\alpha(\tilde \theta_{i}^{\beta} | \theta_{i-1}^{\beta}, \vect \theta_{i-1}^{- \beta})& = 1.
\end{align}

For the parameter $t_1$ we use a uniform random walk proposal distribution,

\begin{align*}
\tilde \theta_{i}^{t_1} \sim \mathrm{Uniform}(\theta_{i-1}^{t_1} - d, \theta_{i-1}^{t_1} + d) \\
Q(\tilde \theta_{i}^{t_1} | \theta_{i-1}^{t_1}, \vect \theta_{i-1}^{- t_1}) = \frac{1}{2d}
\end{align*}

Here $d$ is a tuning parameter. Since the proposal distribution is symmetric around the current value, that is $Q(\tilde \theta_{i}^{t_1} | \theta_{i-1}^{t_1}, \vect \theta_{i-1}^{- t_1}) = Q(\theta_{i-1}^{t_1} | \tilde \theta_{i}^{t_1}, \vect \theta_{i-1}^{- t_1})$, the acceptance probability becomes

\begin{align*}
\alpha(\tilde \theta_{i}^{t_1} | \theta_{i-1}^{t_1}, \vect \theta_{i-1}^{- t_1})& = 
\min \left \{ 1, 
\frac{\pi (\tilde \theta^{t_1} |  \vect \theta_{i-1}^{-t_1})}{\pi (\theta_{i-1}^{t_1} |  \vect \theta_{i-1}^{-t_1})} 
\right \}
= \min \left \{1,
\frac{e^{\tilde t_1 (\lambda_1 - \lambda_0)} \lambda_0^{\tilde{y_0} + 1} \lambda_1^{\tilde{y_1} + 1}}{e^{t_1 (\lambda_1 - \lambda_0)} \lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1}}
\right \} \\
&= \min \left \{1,
e^{(\tilde t_1 - t_1) (\lambda_1 - \lambda_0)} \lambda_0^{\tilde y_0 - y_0} \lambda_1^{\tilde{y_1} - y_1}
\right \}.
\end{align*}

Note that again the iteration index is exluded from the longer expressions for readability. We will consistently do this for the longer expressions for the rest of problem A. To avoid numerical owerflow and underlow and increase efficiency the computations of acceptance probability is computed on log-scale.


```{r}
# function used to sample from full conditional of beta.
# use the fact that 1 / beta is gamma distributed.
sample.beta.full.conditional = function(lambda0, lambda1) {
  return(1 / rgamma(1, shape = 6, rate = (1 + lambda0 + lambda1)))
}

#function used to sample from full conditional of lambda0
sample.lambda0.full.conditional = function(t1, beta,  t0, y) {
  return(rgamma(1, shape = (y[1] + 2), rate = (t1 - t0 + 1 / beta)))
}

#function used to sample from full conditional of lambda1
sample.lambda1.full.conditional = function(t1, beta, t2, y) {
  return(rgamma(1, shape = (y[2] + 2), rate = (t2 - t1 + 1 / beta)))
}

#function used to sample a new t1 value using a uniform random walk with Metropolis-Hastings
# probability for accepting new t1 value. If new step not accepted, returns old value.
#Implemented on log-form for efficiency and avoiding too large or to small values
# d is a tuning parameter that determines how large steps can be taken
sample.t1.random.walk.uniform = function(t1.prev, lambda0, lambda1, d, t0, t2, y.prev, dates) {
  # get proposal for new t1 value from uniform distribution around current value
  t1.proposal = runif(1, t1.prev - d, t1.prev + d)
  
  # if proposal is outside the allowed range, return the current value of t1
  if (t1.proposal <= t0 | t1.proposal >= t2) return(list(t1 = t1.prev, y = y.prev, accepted = 0))
  
  # function returning the not normalized full conditional of t1 on log-form
  log.full.conditional.t1 = function(t1, lambda0, lambda1, y) {
    return(t1 * (lambda1 - lambda0) + (y[1] + 1) * log(lambda0)  + (y[2]+1) * log(lambda1)) 
  }
  
  # find the number of explosions between t0 and t1 (y1) and the number of explosions 
  # between t1 and t2 (y2)
  y.proposal = c(sum(dates <= t1.proposal), sum(dates > t1.proposal))
  
  #calculate acceptance probability
  accept.prob = min(exp(log.full.conditional.t1(t1.proposal, lambda0, lambda1, y.proposal) - 
                          log.full.conditional.t1(t1.prev, lambda0, lambda1, y.prev)), 1)
  
  # get a uniform random number between 0 and 1
  u = runif(1)
  # Return new propsals if u < acceptance probability
  if(u < accept.prob) return(list(t1 = t1.proposal, y = y.proposal, accepted = 1))
  #else return current values
  else return(list(t1 = t1.prev, y = y.prev, accepted = 0))
}
```

```{r}
# Implements a single site mcmc formula for approximating the posterior distribution of the parameters of interest
single.mcmc = function(n, K, dates, d, 
                       t1.initial = runif(1, dates[1], dates[length(dates)]),
                       beta.initial = 1 / rgamma(1, shape = 2, rate = 1),
                       lambda0.initial = 1 / rgamma(1, shape = 3, scale = beta.initial),
                       lambda1.initial = 1 / rgamma(1, shape = 3, scale = beta.initial)) {
  "
  Input:
    -n: number of useful samples after the burn-in period wanted
    -K: length of the burn-in period
    -dates: the dates of the explosions (+ start and end date)
    -d: the maximum step length for the uniform random walk of t1
    -t1.initial: the initial value of t1. If not provided it is sampled from the prior
    -beta.initial: the initial value of beta. If not provided it is sampled from the prior
    -lambda0.initial: the initial value of lambda0. If not provided it is sampled from the
    prior given beta.initial
    -lambda1.initial: the initial value of lambda1. If not provided it is sampled from the
    prior given beta.initial
    
  Output:
    -A list containing
      -- t1: a vector of sampled t1 values
      -- beta: a vector of sampled beta values
      -- lambda0: a vector of sampled lambda0 values
      -- lambda1: a vector of sampled lambda1 values
      -- iter = iter
      -- df.parameters: a data frame containing all of the vectors above as columns in
      addition to a column with boolean value of TRUE if the observation is a part of the
      burn-in period
      -- df.parameters.long: a data frame with the same info as df.parameters, but in long-format
      -- n: number of useful samples after the burn-in period wanted
      -- K: length of the burn-in period
      -- d: the maximum step length for the uniform random walk of t1
      -- percent.t1.accepted: the percent of proposed t1 values accepted
  "
  
  
  K = K + 1 # to make space for initial values
  t0 = dates[1] # extract start date
  t2 = dates[length(dates)] # extract end date
  date.explosions = dates[2:(length(dates) - 1)] # remove start and end date
  # Create vectors to store values
  t1.values = rep(NA, n+K)
  lambda0.values = rep(NA, n+K)
  lambda1.values = rep(NA, n+K)
  beta.values = rep(NA, n+K)
  
  # Add initial values
  t1.values[1] = t1.initial
  lambda0.values[1] = lambda0.initial
  lambda1.values[1] = lambda1.initial
  beta.values[1] = beta.initial
  
  # create y containing y0 and y1
  y = c(sum(date.explosions <= t1.initial), sum(date.explosions > t1.initial)) 
  
  n.accepted = 0 # keep track of number of accepted t1 proposals
  for (i in 2:(n+K)) {
    # simulate new t1 value. Get corresponding new y value
    new.t1.and.y = sample.t1.random.walk.uniform(t1.values[i-1], lambda0.values[i-1], lambda1.values[i-1],
                                                 d, t0, t2, y, date.explosions)
    t1.values[i] = new.t1.and.y$t1
    y = new.t1.and.y$y
    # keep track of number of accepted t1 proposals
    n.accepted = n.accepted + new.t1.and.y$accepted
    # simulate new beta, lambda0 and lambda1 from full conditionals
    beta.values[i] = sample.beta.full.conditional(lambda0.values[i-1], lambda1.values[i-1])
    lambda0.values[i] = sample.lambda0.full.conditional(t1.values[i], beta.values[i],  t0, y)
    lambda1.values[i] = sample.lambda1.full.conditional(t1.values[i], beta.values[i], t2, y)
  }
  
  # Create a vector with the iteration numbers. The initial values correspond to iteration 0.
  iter = c(seq(0, n+K-1))
  
  # Add parameter values and other interesting information to a data frame
  df.parameters = data.frame(iter = iter, t1 = t1.values, beta = beta.values,
                             
                             lambda0 = lambda0.values,
                             lambda1 = lambda1.values, 
                             burn.in = c(rep(TRUE, K), rep(FALSE, n)))
  
  # make the same dataframe on long format
  df.parameters.long = pivot_longer(df.parameters, c(-iter, -burn.in),
                                    
                                    names_to = "parameters", values_to = "value")
  
  # return everything of interest in a list for easy access
  return(list(t1 = t1.values, beta = beta.values, lambda0 = lambda0.values,
              
              lambda1 = lambda1.values, df.parameters = df.parameters,
              
              df.parameters.long = df.parameters.long,
              
              n = n, K = K-1, d = d, iter = iter, 
              
              percent.t1.accepted = n.accepted / (n + K - 1)))
}
```

## 5)

In this task we run our single site algorithm implemented in task A4 and evaluate the burn-in and mixing properties of our algorithm.

```{r run_mcmc_single}
# run the algorithm for intital values sampled from prior
set.seed(1)
single.mc.result = single.mcmc(20000,1000, df.coal$date, 10)
```

```{r fig_captions5, echo = FALSE}
# Creating the figure caption
fig.caption.trace.plot = paste("\\label{fig:trace_plot} Trace plot of the parameters $\\beta, \\lambda_0, \\lambda_1$ and $t_1$ for a burn-in period of 1000 iterations and then 20000 iterations of a hopefully converged single site MCMC algorithm. The initial values was drawn from the priors and was for this realization; $t_1^{\\textrm{initial}} =$ ", round(single.mc.result$t1[1],3), ", $\\beta^{\\textrm{initial}} =$ ", round(single.mc.result$beta[1],3), ", $\\lambda_0^{\\textrm{initial}} =$ ", round(single.mc.result$lambda0[1],3), ", $\\lambda_1^{\\textrm{initial}} =$ ", round(single.mc.result$lambda1[1],3), ". The tuning parameter is $d = 10.$", sep = "")

fig.caption.histogram.plot = paste("\\label{fig:histogram_plot} Histogram of the parameters $\\beta, \\lambda_0, \\lambda_1$ and $t_1$ for 20000 iterations after discarding the 1000 burn-in samples. The initial values was drawn from the priors and was for this realization; $t_1^{\\textrm{initial}} =$ ", round(single.mc.result$t1[1],3), ", $\\beta^{\\textrm{initial}} =$ ", round(single.mc.result$beta[1],3), ", $\\lambda_0^{\\textrm{initial}} =$ ", round(single.mc.result$lambda0[1],3), ", $\\lambda_1^{\\textrm{initial}} =$ ", round(single.mc.result$lambda1[1],3), ". The tuning parameter is $d = 10.$", sep = "")

fig.caption.autocorrelation.plot = paste("\\label{fig:autocorrelation_plot}Autocorrelation plot of the parameters $\\beta, \\lambda_0, \\lambda_1$ and $t_1$ for 20000 iterations after discarding the 1000 burn-in samples. The initial values was drawn from the priors and was for this realization; $t_1^{\\textrm{initial}} =$ ", round(single.mc.result$t1[1],3), ", $\\beta^{\\textrm{initial}} =$ ", round(single.mc.result$beta[1],3), ", $\\lambda_0^{\\textrm{initial}} =$ ", round(single.mc.result$lambda0[1],3), ", $\\lambda_1^{\\textrm{initial}} =$ ", round(single.mc.result$lambda1[1],3), ". The tuning parameter is $d = 10.$", sep = "")
```


```{r, fig.width = 10, fig.height = 4, eval.after = "fig.cap", fig.cap = fig.caption.trace.plot, dependson=c("fig_captions5", "run_mcmc_single")}
# Plot trace plots of each parameter for a single realization of the single site MCMC
ggplot(single.mc.result$df.parameters.long, aes(x = iter, y = value)) + geom_line() +
  facet_wrap(~parameters, scale = "free_y") +
  geom_vline(aes(xintercept = single.mc.result$K, col = "Cut off point burn in period"), size = 1) +
  theme(legend.position="bottom") + guides(col = guide_legend("Line")) +
  xlab("Iteration") + ylab("Value") +
  
  ggtitle("Trace plot of the parameters")
```

From Figure \ref{fig:trace_plot} it appears that the MCMC output has converged to the posterior distribution according to the trace plot. The samples forming a homogene band after the burn-in period (and some time before) indicates convergence. Another positive observations is that the chain seem to move fast through the posterior sample space, indicating low correlation between subsequent samples.

```{r, fig.width = 10, fig.height = 4, fig.cap = fig.caption.histogram.plot, dependson="fig_caption5"}
# Plot histograms of each parameter for a single realization of the single site MCMC
ggplot(subset(single.mc.result$df.parameters.long, burn.in == FALSE),
       aes(x = value, y = ..density..)) + geom_histogram(colour = "lightgrey") +
  facet_wrap(~parameters, scale = "free") + ggtitle("Histogram of parameter values after burn-in period")
```

```{r, echo = FALSE}
t1.assumed = 1890
t0 = df.coal$date[1]
t2 = df.coal$date[nrow(df.coal)]
date.explosions = df.coal$date[2:nrow(df.coal)]
lambda0.estimated = sum(date.explosions <= t1.assumed) / (t1.assumed-t0)
lambda1.estimated = sum(date.explosions > t1.assumed) / (t2-t1.assumed)
```


From Figure \ref{fig:1a_cumplot} it appears that there is a large change in rate of accidents around 1890, and a smaller around 1945. Consequently, since the histogram plot of parameter $t_1$ in Figure \ref{fig:histogram_plot} is centered around 1890 and has most of it's samples a distance of plus-minus five years away, it indicates that the simulated values of $t_1$ are reasonable. If we assume that 1890 is the correct value of $t_1$ and calulcate the average rate of accidents between $t_0$ and 1890 then we get a rate of $`r round(lambda0.estimated,3)`$. Similarily, if we calculate the average rate of accidents between $t_0$ and 1890 then we get a rate of $`r round(lambda1.estimated,3)`$. Comparing these values against the corresponding histograms of simulated values in Figure \ref{fig:histogram_plot} it appears that the simulated values of $\lambda_0$ and $\lambda_1$ are reasonable.

Another diagnostic tool is to examine dependencies of successive MCMC samples. From the autocorrelation plots in Figure \ref{fig:autocorrelation_plot} we observed that for lag 20 and higher there are almost zero correlation between the samples for $t_1$. Additionally, the correlation is almost zero already for lag 5 for both $\lambda_0$ and $\lambda_1$ samples and lag 3 for $\beta$ samples. 

```{r, fig.width=8, fig.height=4, fig.ncol = 2, out.width = "50%", fig.show = "hold", fig.cap = fig.caption.autocorrelation.plot, dependson="fig_caption5"}
df.parameters.after.burn.in = subset(single.mc.result$df.parameters, burn.in == FALSE)
# Plotting the autocorrelation functions for the parameter samples
acf(df.parameters.after.burn.in$t1, main = TeX("Autocorrelation for $t_1$ samples"))
acf(df.parameters.after.burn.in$beta, main = TeX("Autocorrelation for $\\beta$ samples"))
acf(df.parameters.after.burn.in$lambda0, main = TeX("Autocorrelation for $\\lambda_0$ samples"))
acf(df.parameters.after.burn.in$lambda1, main = TeX("Autocorrelation for $\\lambda_1$ samples"))
```


```{r}
# Finding estimates for effective sample size
effective.size.single.site.mcmc = effectiveSize(as.mcmc(df.parameters.after.burn.in[, c("t1", "beta", "lambda0", "lambda1")]))
effective.size.single.site.mcmc
```


A useful measure to compare the performance of MCMC samplers is the effective sample size (ESS).

$$ \mathrm{ESS} = \frac{n}{\tau}, \qquad \tau = 1 + 2 \sum_{k=1}^\infty \rho(k),$$
where $\tau$ is the autocorrelation time and $\rho(k)$ the autocorrelation at lag $k$. The estimated effective samples size of $t_1, \beta, \lambda_0$ and $\lambda_1$ is respectively $`r round(effective.size.single.site.mcmc[1], 2)`$, $`r round(effective.size.single.site.mcmc[2], 2)`$, $`r round(effective.size.single.site.mcmc[3], 2)`$ and $`r round(effective.size.single.site.mcmc[4], 2)`$. In compliance with the autocorrelation plots, the effective sample size of $t_1$ is much smaller than the other parameters. This is due to the samples of $t_1$ being much more correlated.

The diagnostics above was done only for a single MCMC realization. To increase our certainty that the chain actually has converged to the posterior distrbution we will try to run it again from multiple initial values far away from the values it converged to above.

```{r fig_captions5_multiple, echo = FALSE}
# Creating the figure caption
fig.caption.trace.plot.multiple = paste("\\label{fig:trace_plot_multiple} Trace plot of the realizations of the parameters $\\beta, \\lambda_0, \\lambda_1$ and $t_1$ for two different sets of initial values. Each of the chains are run for 20000 iterations with tuning parameter $d = 10$. The initial values for realization 2 were; $t_1^{\\textrm{initial}} =$ ", 1960, ", $\\beta^{\\textrm{initial}} =$ ", 5 , ", $\\lambda_0^{\\textrm{initial}} =$ ", 10, ", $\\lambda_1^{\\textrm{initial}} =$ ", 10, ". The initial values for realization 3 were; $t_1^{\\textrm{initial}} =$ ", 1855, ", $\\beta^{\\textrm{initial}} =$ ", 5 , ", $\\lambda_0^{\\textrm{initial}} =$ ", 0.1, ", $\\lambda_1^{\\textrm{initial}} =$ ", 10, ".",sep = "")
```


```{r, fig.width=10, fig.height = 5, dependson="fig_captions5_multiple", fig.cap = fig.caption.trace.plot.multiple}
single.mc.result1 = single.mcmc(20000,0, df.coal$date, 10, t1.initial = 1960, beta.initial = 5, lambda0.initial = 10, lambda1.initial = 10)
single.mc.result2 = single.mcmc(20000,0, df.coal$date, 10, t1.initial = 1855, beta.initial = 5, lambda0.initial = 0.1, lambda1.initial = 10)

set.seed(1)
df.parameters.single.mc.multiple = rbind(single.mc.result1$df.parameters.long, single.mc.result2$df.parameters.long)
df.parameters.single.mc.multiple$realization = c(rep("1", nrow(single.mc.result1$df.parameters.long)), rep("2", nrow(single.mc.result2$df.parameters.long)))

# Plot trace plots of each parameter for a single realization of the single site MCMC
ggplot(df.parameters.single.mc.multiple, aes(x = iter, y = value, col = realization)) + geom_line(alpha = 0.75) +
  facet_wrap(~parameters, scale = "free_y") +
  theme(legend.position="bottom") + guides(col = guide_legend("Lines")) +
  xlab("Iteration") + ylab("Value") + 
  
  ggtitle("Trace plot of the parameters for two differently chosen initial conditions") +
  scale_color_manual(values = c("black", "lightblue", "red"), labels = c("Realization 2", "Realization 3", "Vertical line at 1000 iterations")) + geom_vline(aes(xintercept = 1000, col = "Vertical line at 1000 iterations"), alpha = 0.5)
```


In Figure \ref{fig:trace_plot_multiple} we see an example where the burn-in of 1000 iterations is barely enough to converge to the posterior distribution. For the previous iterations it is stuck at a local mamxima for $t_1$ around 1940. This illustrates the importance of running the MCMC algorithm long enough and from multiple initial conditions to determine if convergence has actually been reached. In the end these two new realizations seem to converge to the same distribution, supporting our belief that we for the first realization reached convergence during our burn-in period. Based on the three realizations the mixing properties of the algorithm appear quite good, as the Markov chain reaches the posterior fairly quickly and moves quickly around the posterior modes. The very low autocorrelation for lag larger than 10 that we observe is another indication that the algorithm mixes well.


## 6)

For this task we explore how the tuning parameter $d$ influences the length of the burn-in and the mixing properties of the simulated Markov chain. To do this we will run the algorithm for several different $d$-values and from three different starting locations. The starting locations will be drawn randomly from the priors, but by setting a seed for each one we will ensure the same three intial values for each $d$-value.


```{r multiple_d_function}
# set the d-values which are to be evaluated. 
# Divide into two list to be able to get output on two pages
d.values.low = c(1,3,5)
d.values.high = c(10,20,100)
# set number of iterations performed
n = 20000
K = 0
# set the different seeds for the initial values. 
# Chosen such that t_1 begins with a low, medium and high value
seeds = c(12,4,7)

# A function that runs the MCMC single site algorithm and plots.
# Returns a string to be used as figure caption
plot.for.mutiple.d = function(n, K, d.values, fig.label) {
  for (d in d.values) {
    # set seeds and run for three different initial conditions
    set.seed(seeds[1])
    single.mc.test.d.1 = single.mcmc(n,K, df.coal$date, d = d)
    set.seed(seeds[2])
    single.mc.test.d.2 = single.mcmc(n,K, df.coal$date, d = d)
    set.seed(seeds[3])
    single.mc.test.d.3 = single.mcmc(n,K, df.coal$date, d = d)
    # Combine the realizations to a single data frame for plotting
    df.parameters.test.d = rbind(single.mc.test.d.1$df.parameters.long,
                                    single.mc.test.d.2$df.parameters.long,
                                    single.mc.test.d.3$df.parameters.long)
    # add a column that keeps track of which realizaiton the observations belong to
    df.parameters.test.d$realization = rep(c("1","2","3"),
                                           each = nrow(single.mc.test.d.1$df.parameters.long))
    # create the plot
    p = ggplot(df.parameters.test.d, aes(x = iter, y = value, col = realization)) +
      
      geom_line(alpha = 0.75) + facet_wrap(~parameters, scale = "free_y", ncol = 4) +
      theme(legend.position="bottom") + guides(col = guide_legend("Realizations")) +
      xlab("Iteration") + ylab("Value") +
  
      ggtitle(paste("Trace plot of the parameters for three differently chosen",
                    "initial conditions and d =", d),
              subtitle = TeX(paste("Average acceptance probability for $t_1$:", 
                               round((single.mc.test.d.1$percent.t1.accepted +
                                        single.mc.test.d.2$percent.t1.accepted +
                                        single.mc.test.d.3$percent.t1.accepted) / 3,
                                      3)))) +
      scale_color_manual(values = c("black", "lightblue", "lightgrey"), labels = c("1", "2", "3"))
    # print the plot
    print(p)
  }
  
  # create figure caption. Written in a specific format to be able to show everythin in rmarkdown
fig.caption.d = paste("\\label{", fig.label, "}Trace plot of the parameters for tuning ",

    "parameter $d \\in [$", toString(d.values.low), "$]$ and three different ",
    
    "initial conditions. The single site MCMC algorithm was run for ", n, " iterations. ",
    
    "The initial values for realization 1 were; $t_1^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.1$t1[1],2), ", $\\beta^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.1$beta[1],2) , ", $\\lambda_0^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.1$lambda0[1],2), ", $\\lambda_1^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.1$lambda1[1],2), ". ",

    "The initial values for realization 2 were; $t_1^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.2$t1[1],2), ", $\\beta^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.2$beta[1],2) , ", $\\lambda_0^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.2$lambda0[1],2), ", $\\lambda_1^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.2$lambda1[1],2), ". ",

    "The initial values for realization 3 were; $t_1^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.3$t1[1],2), ", $\\beta^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.3$beta[1],2) , ", $\\lambda_0^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.3$lambda0[1],2), ", $\\lambda_1^{\\textrm{initial}} =$ ",
    
    round(single.mc.test.d.3$lambda1[1],2), ".", sep = "")
  # return figure caption
  return(fig.caption.d)
}
```


```{r, fig.width = 10, fig.height = 3, out.width = "100%", dependson = "multiple_d_function", fig.show = "hold", fig.cap = fig.caption.d.low, eval.after = "fig.cap"}
# plot for the low d values
fig.caption.d.low = plot.for.mutiple.d(n, K, d.values.low, fig.label = "fig:trace_plot_low_d")
```



```{r fig_caption_d, fig.width = 10, fig.height = 3, out.width="100%", fig.show = "hold", eval.after = "fig.cap", dependson = "multiple_d_function", fig.cap = fig.caption.d.high}
# plot for the high d values
fig.caption.d.high = plot.for.mutiple.d(n, K, d.values.high, fig.label = "fig:trace_plot_high_d")
```


From Figure \ref{fig:trace_plot_low_d} we observe that for $d \leq 5$ the algorithm seem to take a lot of time to get out of a local maximima due to starting with initial conditions far away from the likely values according to the posterior distributions. For $d = 5$ the burn-in period appear to end after 3000 iterations, while for $d = 3$ and $d = 1$ the chains seem to still be in the burn-in period after 20000 iterations for the realizations starting with initial $t_1$-values as $1960$. From Figure \ref{fig:trace_plot_low_d} and \ref{fig:trace_plot_high_d} we observe a trend that higher $d$-values result in shorter burn-in periods. For $d = 10$ the burn-in period appear to usually be over in less than or equal to 1000 iterations, while $d = 20$ and $d = 100$ reach convergence even faster. Although convergence is reached faster for higher $d$-values we see that there is a trade-off against acceptance probability where higher $d$ also decreases the effective sample size due to higher correlation. By inspection, our initial choice of $d = 10$ therefore seem to be a reasonable choice with an burn-in period usually around 1000 iterations and an acceptance probability of $t_1$ around 0.25. Figure \ref{fig:trace_plot_low_d} and \ref{fig:trace_plot_high_d} support our belief (and theory) that the limiting distribution is not influenced by the tuning parameter, just how fast it converges.

## 7)

In this task we define and implement a block Metropolis–Hastings algorithm for $\pi(\vect \theta | \vect x)$ using the two block proposals defined below. Each block proposal will take turns being proposed for each iteration, i.e block proposal 1 will be used for all odd iterations and block proposal 2 will be used for all even iterations. As mentioned earlier, to increase readability we will write out expressions for the full conditionals, proposal distributions and acceptance probabilites without corresponding iteration indexes.


### Block proposal 1) 
The first block proposal is a block proposal for $(t_1, \lambda_0, \lambda_1)$ keeping $\beta = \theta^\beta$ unchanged. We generate the potential new values $(\tilde t_1, \tilde \lambda_0, \tilde \lambda_1) = \tilde \theta^{- \beta}$ by first generating $\tilde t_1 = \tilde \theta ^{t_1})$ from a normal distribution centered at the current value of $t_1$ and thereafter generate $(\tilde \lambda_0, \tilde \lambda_1) = (\tilde \theta^{\lambda_0}, \tilde \theta^{\lambda_1}$ from their joint full conditionals inserted the potential new value $\tilde t_1$, i.e $\pi(\lambda_0, \lambda_1 | \vect x, \tilde t_1, \beta)$.

First we want to find the joint posterior distribution that will be used to calculate the acceptance probability of the block proposal. As $\lambda_0$ and $\lambda_1$ are independent given $\beta$ and $t_1$ their joint full conditional is

\begin{align*}
\pi(\lambda_0, \lambda_1 | \vect x, t_1, \beta)
&= \pi(\lambda_0 | \vect{x}, t_1, \lambda_1, \beta) \pi(\lambda_1 | \vect{x}, t_1, \lambda_0, \beta) \\
&\propto \lambda_0^{y_0 + 1} e^{-\lambda_0(t_1 - t_0 + \frac{1}{\beta})}  \lambda_1^{y_1 + 1} e^{-\lambda_1(t_2 - t_1 + \frac{1}{\beta})}, \qquad  \lambda_0 \geq 0, \lambda_1 \geq 0.
\end{align*}

Using that the joint conditional of $(t_1, \lambda_0, \lambda_1)$ is proportional to the full posterior, we get that their joint full conditional is proportional to

\begin{align*}
\pi(t_1, \lambda_0, \lambda_1 | \vect x, \beta)
&= \frac{\pi(\lambda_0, \lambda_1, t_1, \beta | \vect x)}{\pi(\beta | \vect x)}
\propto \pi(\lambda_0, \lambda_1, t_1, \beta | \vect x) \\
&\propto e^{-\lambda_0 (t_{1} - t_0) - \lambda_1 (t_{2} - t_1) - \frac{\lambda_0 + \lambda_1 + 1}{\beta}}
\lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1}, \qquad  \lambda_0 \geq 0, \lambda_1 \geq 0, t_1 \in [t_0, t_2].
\end{align*}

Next we specify the proposal distributions. The proposal distribution of $t_1$ is a normally distributed random walk

\begin{align*}
\tilde \theta^{t_1} &\sim \mathcal{N} (\theta_{i-1}^{t_1}, \sigma_{t_1}^2) \\
 Q(\tilde \theta_i^{t_1} | \theta_{i-1}^{t_1}, \vect \theta_{i-1}^{-t_1}) &=
 \frac{1}{\sqrt{2 \pi} \sigma_{t_1}} e^{\frac{-1}{2 \sigma_{t_1}^2} (\tilde t_1 - t_1)^2 }
\end{align*}

The proposal function for $\lambda_0$ and $\lambda_1$ is the joint full conditional and due to the independence of $\lambda_0$ and $\lambda_1$ given $\beta$ and $t_1$, the proposal functions are still given by

\begin{align*}
Q(\tilde \theta_{i}^{\lambda_0} | \theta_{i-1}^{\lambda_0}, \tilde \theta_{i}^{t_1}, \vect \theta_{i-1}^{- \{\lambda_0, t_1\}})
& = \pi (\tilde \theta_{i}^{\lambda_0} | \tilde \theta_i^{t_1}, \vect \theta_{i-1}^{- \{\lambda_0, t_1\}}) 
= \frac{(\tilde t_1 - t_0 + \frac{1}{\beta})^{\tilde y_0+2}}{\Gamma(\tilde y_0+2)}
\tilde \lambda_0^{\tilde y_0 + 1} e^{-\tilde \lambda_0(\tilde t_1 - t_0 + \frac{1}{\beta})} \\
Q(\tilde \theta_{i}^{\lambda_1} | \theta_{i-1}^{\lambda_1}, \tilde \theta_{i}^{t_1}, \vect \theta_{i-1}^{- \{\lambda_1, t_1\}})
& = \pi (\tilde \theta_{i}^{\lambda_1} | \tilde \theta_i^{t_1}, \vect \theta_{i-1}^{- \{\lambda_1, t_1\}})
= \frac{(t_2 - \tilde t_1 + \frac{1}{\beta})^{\tilde y_1+2}}{\Gamma(\tilde y_1+2)}
\tilde \lambda_1^{\tilde y_1 + 1} e^{-\tilde \lambda_1(t_2 - \tilde t_1 + \frac{1}{\beta})}.
\end{align*}

Here $\vect \theta_{i-1}^{\{ j,k\}} = [\theta_i^1, \dots, \theta_{i}^{j-1}, \theta_{i-1}^{j+1}, \dots, \theta_{i-1}^{k-1}, \theta_{i-1}^{k+1}, \theta_{i-1}^4]$. 
The block acceptance probability becomes


\begin{align*}
\alpha (\tilde{\vect \theta_i}^{-\beta} | \theta_i^{\beta})
&=
\min \left \{1,
\frac{\pi(\tilde t_1, \tilde \lambda_0, \tilde \lambda_1 | \beta)}{\pi(t_1, \lambda_0, \lambda_1 |, \beta)}
\frac{Q(t_1, \lambda_0, \lambda_1 | \tilde t_1, \tilde \lambda_0, \tilde \lambda_1, \beta)}
{Q(\tilde t_1 \tilde \lambda_0, \tilde \lambda_1| t_1, \lambda_0, \lambda_1 \beta)}
\right \} \\
&=
\min \left \{1,
\frac{\pi(\tilde t_1, \tilde \lambda_0, \tilde \lambda_1 | \beta)}{\pi(t_1, \lambda_0, \lambda_1 |, \beta)}
\frac{Q(t_1| \tilde t_1)}
{Q(\tilde t_1 | t_1)}
\frac{Q(\lambda_0, \lambda_1 | t_1, \beta)}
{Q(\tilde \lambda_0, \tilde \lambda_1| \tilde t_1, \beta)}
\right \} \\
&=
\min \Bigg \{1,
\frac{e^{- \tilde \lambda_0 (\tilde t_{1} - t_0) - \tilde \lambda_1 (t_{2} - \tilde t_1) - \frac{\tilde \lambda_0 + \tilde \lambda_1 + 1}{\beta}}
\tilde \lambda_0^{\tilde y_0 + 1} \tilde \lambda_1^{\tilde y_1 + 1}}
{e^{-\lambda_0 (t_{1} - t_0) - \lambda_1 (t_{2} - t_1) - \frac{\lambda_0 + \lambda_1 + 1}{\beta}}
\lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1}}
\frac{e^{\frac{-1}{2 \sigma_{t_1}^2} (t_1 - \tilde t_1)^2 }}{e^{\frac{-1}{2 \sigma_{t_1}^2} (\tilde t_1 -t_1)^2 }} \\
& \hspace{1.7cm} \cdot
\frac
{\frac{( t_1 - t_0 + \frac{1}{\beta})^{ y_0+2}}{\Gamma(y_0+2)}
\lambda_0^{ y_0 + 1} e^{- \lambda_0( t_1 - t_0 + \frac{1}{\beta})}}
{\frac{(\tilde t_1 - t_0 + \frac{1}{\beta})^{\tilde y_0+2}}{\Gamma(\tilde y_0+2)}
\tilde \lambda_0^{\tilde y_0 + 1} e^{-\tilde \lambda_0(\tilde t_1 - t_0 + \frac{1}{\beta})}}
\frac
{\frac{(t_2 - t_1 + \frac{1}{\beta})^{y_1+2}}{\Gamma( y_1+2)}
\lambda_1^{ y_1 + 1} e^{- \lambda_1(t_2 - t_1 + \frac{1}{\beta})}}
{\frac{(t_2 - \tilde t_1 + \frac{1}{\beta})^{\tilde y_1+2}}{\Gamma(\tilde y_1+2)}
\tilde \lambda_1^{\tilde y_1 + 1} e^{-\tilde \lambda_1(t_2 - \tilde t_1 + \frac{1}{\beta})}} 
\Bigg \} \\
&=
\min \Bigg \{1,
\frac{e^{- \tilde \lambda_0 (\tilde t_{1} - t_0) - \tilde \lambda_1 (t_{2} - \tilde t_1) - \frac{\tilde \lambda_0 + \tilde \lambda_1 + 1}{\beta}}
\tilde \lambda_0^{\tilde y_0 + 1} \tilde \lambda_1^{\tilde y_1 + 1}}
{e^{-\lambda_0 (t_{1} - t_0) - \lambda_1 (t_{2} - t_1) - \frac{\lambda_0 + \lambda_1 + 1}{\beta}}
\lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1}} \\
& \hspace{1.7cm} \cdot
\frac
{\frac{( t_1 - t_0 + \frac{1}{\beta})^{ y_0+2}}{\Gamma(y_0+2)}
\lambda_0^{ y_0 + 1} e^{- \lambda_0( t_1 - t_0 + \frac{1}{\beta})}}
{\frac{(\tilde t_1 - t_0 + \frac{1}{\beta})^{\tilde y_0+2}}{\Gamma(\tilde y_0+2)}
\tilde \lambda_0^{\tilde y_0 + 1} e^{-\tilde \lambda_0(\tilde t_1 - t_0 + \frac{1}{\beta})}}
\frac
{\frac{(t_2 - t_1 + \frac{1}{\beta})^{y_1+2}}{\Gamma( y_1+2)}
\lambda_1^{ y_1 + 1} e^{- \lambda_1(t_2 - t_1 + \frac{1}{\beta})}}
{\frac{(t_2 - \tilde t_1 + \frac{1}{\beta})^{\tilde y_1+2}}{\Gamma(\tilde y_1+2)}
\tilde \lambda_1^{\tilde y_1 + 1} e^{-\tilde \lambda_1(t_2 - \tilde t_1 + \frac{1}{\beta})}} 
\Bigg \} \\
&=
\min \Bigg \{1,
\frac
{\frac{( t_1 - t_0 + \frac{1}{\beta})^{ y_0+2}}{\Gamma(y_0+2)}}
{\frac{(\tilde t_1 - t_0 + \frac{1}{\beta})^{\tilde y_0+2}}{\Gamma(\tilde y_0+2)}}
\frac
{\frac{(t_2 - t_1 + \frac{1}{\beta})^{y_1+2}}{\Gamma( y_1+2)}}
{\frac{(t_2 - \tilde t_1 + \frac{1}{\beta})^{\tilde y_1+2}}{\Gamma(\tilde y_1+2)}}
\Bigg \}
\end{align*}

To avoid numerical issues and increase efficience the computations of block acceptance probability is in the code performed on log-scale.

### Block proposal 2)

The second block-proposal is defined as follows. A block proposal for $(\beta_0, \lambda_0, \lambda_1) = \theta^{-t_1}$ keeping $t_1$ unchanged. We generate the potential new values $(\tilde \beta_0, \tilde \lambda_0, \tilde \lambda_1) = \tilde \theta^{-t_1}$ by first generating $\tilde \beta = \tilde \theta^{\beta}$ from a normal distribution centered at the current value $\beta$ and thereafter generate $(\tilde \lambda_0, \tilde \lambda_1)$ from their resulting joint full conditional inserted $\tilde \beta$, i.e $\pi (\tilde \lambda_0, \tilde \lambda_1 | \vect x, t_1, \tilde \beta)$.

The proposal distribution of $\beta$ is a normally distributed random walk

\begin{align*}
\tilde \theta^{\beta} &\sim \mathcal{N} (\theta_{i-1}^{\beta}, \sigma_{\beta}^2) \\
 Q(\tilde \theta_i^{\beta} | \theta_{i-1}^{\beta}, \vect \theta_{i-1}^{-\beta}) &=
 \frac{1}{\sqrt{2 \pi} \sigma_{\beta}} e^{\frac{-1}{2 \sigma_{\beta}^2} (\tilde \beta - \beta)^2 }.
\end{align*}

Using the same reasoning as in task A7, we get that since $\lambda_0$ and $\lambda_1$ are independent given $\beta$ and $t_1$ their joint full conditional is

\begin{align*}
\pi(\lambda_0, \lambda_1 | \vect x, t_1, \beta)
&= \pi(\lambda_0 | \vect{x}, t_1, \lambda_1, \beta) \pi(\lambda_1 | \vect{x}, t_1, \lambda_0, \beta) \\
&\propto \lambda_0^{y_0 + 1} e^{-\lambda_0(t_1 - t_0 + \frac{1}{\beta})}  \lambda_1^{y_1 + 1} e^{-\lambda_1(t_2 - t_1 + \frac{1}{\beta})}, \qquad  \lambda_0 \geq 0, \lambda_1 \geq 0.
\end{align*}

Using that the joint conditional of $(\beta, \lambda_0, \lambda_1)$ is proportional to the full posterior, we get that their joint full conditional is proportional to

\begin{align*}
\pi(\beta, \lambda_0, \lambda_1 | \vect x, t_1)
&= \frac{\pi(\lambda_0, \lambda_1, t_1, \beta | \vect x)}{\pi(t_1 | \vect x)}
\propto \pi(\lambda_0, \lambda_1, t_1, \beta | \vect x) \\
&\propto e^{-\lambda_0 (t_{1} - t_0) - \lambda_1 (t_{2} - t_1) - \frac{\lambda_0 + \lambda_1 + 1}{\beta}}
\lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1} \beta^{-5}
, \qquad  \lambda_0 \geq 0, \lambda_1 \geq 0, \beta > 0.
\end{align*}


The proposal function for $\lambda_0$ and $\lambda_1$ is the joint full conditional and due to the independence of $\lambda_0$ and $\lambda_1$ given $\beta$ and $t_1$, the proposal functions are still given by

\begin{align*}
Q(\tilde \theta_{i}^{\lambda_0} | \theta_{i-1}^{\lambda_0}, \tilde \theta_{i}^{\beta}, \vect \theta_{i-1}^{- \{\lambda_0, \beta\}})
& = \pi (\tilde \theta_{i}^{\lambda_0} | \tilde \theta_i^{\beta}, \vect \theta_{i-1}^{- \{\lambda_0, \beta\}}) 
= \frac{(t_1 - t_0 + \frac{1}{\tilde \beta})^{y_0+2}}{\Gamma(y_0+2)}
\tilde \lambda_0^{y_0 + 1} e^{-\tilde \lambda_0(t_1 - t_0 + \frac{1}{\tilde \beta})} \\
Q(\tilde \theta_{i}^{\lambda_1} | \theta_{i-1}^{\lambda_1}, \tilde \theta_{i}^{\beta}, \vect \theta_{i-1}^{- \{\lambda_1, \beta \}})
& = \pi (\tilde \theta_{i}^{\lambda_1} | \tilde \theta_i^{\beta}, \vect \theta_{i-1}^{- \{\lambda_1, \beta \}})
= \frac{(t_2 - t_1 + \frac{1}{\tilde \beta})^{y_1+2}}{\Gamma( y_1+2)}
\tilde \lambda_1^{y_1 + 1} e^{-\tilde \lambda_1(t_2 - t_1 + \frac{1}{\tilde \beta})}.
\end{align*}

The block acceptance probability becomes

\begin{align*}
\alpha (\tilde{\vect \theta_i}^{-t_1} | \theta_i^{t_1})
&=
\min \left \{1,
\frac{\pi(\tilde \beta, \tilde \lambda_0, \tilde \lambda_1 | t_1)}{\pi(\beta, \lambda_0, \lambda_1 |, t_1)}
\frac{Q(\beta, \lambda_0, \lambda_1 | \tilde \beta, \tilde \lambda_0, \tilde \lambda_1, t_1)}
{Q(\tilde \beta, \tilde \lambda_0, \tilde \lambda_1| \beta, \lambda_0, \lambda_1 t_1)}
\right \} \\
&=
\min \left \{1,
\frac{\pi(\tilde \beta, \tilde \lambda_0, \tilde \lambda_1 | t_1)}{\pi(\beta, \lambda_0, \lambda_1 |, t_1)}
\frac{Q(\beta| \tilde \beta)}
{Q(\tilde \beta | \beta)}
\frac{Q(\lambda_0, \lambda_1 | t_1, \beta)}
{Q(\tilde \lambda_0, \tilde \lambda_1| t_1, \tilde \beta)}
\right \} \\
&=
\min \Bigg \{1,
\frac
{e^{-\tilde \lambda_0 (t_{1} - t_0) - \tilde \lambda_1 (t_{2} - t_1) - \frac{\tilde \lambda_0 + \tilde \lambda_1 + 1}{\tilde \beta}}
\tilde \lambda_0^{y_0 + 1} \tilde \lambda_1^{y_1 + 1} \tilde \beta^{-5}}
{e^{-\lambda_0 (t_{1} - t_0) - \lambda_1 (t_{2} - t_1) - \frac{\lambda_0 + \lambda_1 + 1}{\beta}}
\lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1} \beta^{-5}}
\frac{e^{\frac{-1}{2 \sigma_{\beta}^2} (\beta - \tilde \beta)^2 }}{e^{\frac{-1}{2 \sigma_{\beta}^2} (\tilde \beta - \beta)^2 }} \\
&\hspace{1.7cm} \cdot
\frac
{\frac{( t_1 - t_0 + \frac{1}{\beta})^{ y_0+2}}{\Gamma(y_0+2)}
\lambda_0^{ y_0 + 1} e^{- \lambda_0( t_1 - t_0 + \frac{1}{\beta})}}
{\frac{(t_1 - t_0 + \frac{1}{\beta})^{y_0+2}}{\Gamma(y_0+2)}
\tilde \lambda_0^{y_0 + 1} e^{-\tilde \lambda_0(t_1 - t_0 + \frac{1}{\beta})}}
\frac
{\frac{(t_2 - t_1 + \frac{1}{\tilde \beta})^{y_1+2}}{\Gamma( y_1+2)}
\lambda_1^{ y_1 + 1} e^{- \lambda_1(t_2 - t_1 + \frac{1}{\tilde \beta})}}
{\frac{(t_2 - t_1 + \frac{1}{\beta})^{y_1+2}}{\Gamma(y_1+2)}
\tilde \lambda_1^{y_1 + 1} e^{-\tilde \lambda_1(t_2 -  t_1 + \frac{1}{\tilde \beta})}} 
\Bigg \} \\
&=
\min \Bigg \{1,
\frac
{e^{-\tilde \lambda_0 (t_{1} - t_0) - \tilde \lambda_1 (t_{2} - t_1) - \frac{\tilde \lambda_0 + \tilde \lambda_1 + 1}{\tilde \beta}}
\tilde \lambda_0^{y_0 + 1} \tilde \lambda_1^{y_1 + 1} \tilde \beta^{-5}}
{e^{-\lambda_0 (t_{1} - t_0) - \lambda_1 (t_{2} - t_1) - \frac{\lambda_0 + \lambda_1 + 1}{\beta}}
\lambda_0^{y_0 + 1} \lambda_1^{y_1 + 1} \beta^{-5}} \\
& \hspace{1.7cm} \cdot
\frac
{\frac{( t_1 - t_0 + \frac{1}{\beta})^{ y_0+2}}{\Gamma(y_0+2)}
\lambda_0^{ y_0 + 1} e^{- \lambda_0(t_1 - t_0 + \frac{1}{\beta})}}
{\frac{(t_1 - t_0 + \frac{1}{\tilde \beta})^{y_0+2}}{\Gamma(y_0+2)}
\tilde \lambda_0^{y_0 + 1} e^{-\tilde \lambda_0( t_1 - t_0 + \frac{1}{\beta})}}
\frac
{\frac{(t_2 - t_1 + \frac{1}{\beta})^{y_1+2}}{\Gamma( y_1+2)}
\lambda_1^{ y_1 + 1} e^{- \lambda_1(t_2 - t_1 + \frac{1}{\tilde \beta})}}
{\frac{(t_2 - t_1 + \frac{1}{\tilde \beta})^{ y_1+2}}{\Gamma(y_1+2)}
\tilde \lambda_1^{y_1 + 1} e^{-\tilde \lambda_1(t_2 -  t_1 + \frac{1}{\tilde \beta})}}
\Bigg \} \\
&=
\min \Bigg \{1,
\frac{e^{\frac{-1}{\tilde \beta}} \tilde \beta ^{-5}} {e^{\frac{-1}{\beta}} \beta ^{-5}}
\left ( \frac{t_1 - t_0 + \frac{1}{\beta}} {t_1 - t_0 + \frac{1}{\tilde \beta}}\right)^{y_0+2}
\left ( \frac{t_2 - t_1 + \frac{1}{\beta}} {t_2 - t_1 + \frac{1}{\tilde \beta}}\right)^{y_1+2}
\Bigg \}
\end{align*}

Again, to avoid numerical issues and increase efficience the computations of block acceptance probability is in the code performed on log-scale.


```{r}
#function used to sample a new t1, lambda0 and lambda1 value using a block proposal.
# First a new t1 is proposed from a normal distribution centered at the current value of t1.
# Thereafter lambda0 and lambda1 are generated from their joint full conditionals given
# beta and proposed t1.
# Lastly the proposals are either accepted or rejected. If rejected, the current values are returned.
# Implemented on log-form for efficiency and avoiding too large or to small values
# sigma is a tuning parameter that determines how large steps sizes are usually taken by t1
sample.block.t1.normal.walk.and.lambdas = function(t1.prev, lambda0.prev, lambda1.prev, beta,
                                                   sigma, t0, t2, y.prev, dates) {
  # get proposal for new t1 value from normal distribution around current value
  t1.proposal = rnorm(1, mean = t1.prev, sd = sigma)
  
  #print(paste("t1.prev=", t1.prev, "t0=", t0, "t2=", t2, "t1.proposal=", t1.proposal))
  # if proposal is outside the allowed range, return the current value of t1, lambda0 and lambda1
  if (t1.proposal <= t0 | t1.proposal >= t2) return(list(t1 = t1.prev, lambda0 = lambda0.prev,
                                                         lambda1 =lambda1.prev,
                                                         y = y.prev, accepted = 0))

  # find new y for given t1 propsal
  y.proposal = c(sum(dates <= t1.proposal), sum(dates > t1.proposal))
  # get proposal for lambda0 and lambda1 from joint conditional given t1.proposal and beta
  lambda0.proposal = sample.lambda0.full.conditional(t1.proposal, beta, t0, y.proposal)
  lambda1.proposal = sample.lambda1.full.conditional(t1.proposal, beta, t2, y.proposal)
  

  # function returning the not normalized joint conditional of t1, lambda0 and lambda1 on log-form
  log.joint.conditional.t1.lambda0.lambda1 = function(t1, lambda0, lambda1, y) {
    return(- lambda0 * (t1 - t0) - lambda1 * (t2 - t1) - (lambda0 + lambda1 + 1) / beta +
             (y[1] + 1) * log(lambda0) + (y[2] + 1) * log(lambda1)) 
  }
  
  # function returning the full conditional of lambda0 on log-form
  log.gamma.lambda0 = function(t1, lambda0, beta, y0, t0) {
    return(
      (y0+2) * log(t1 - t0 + 1 / beta) - lgamma(y0 + 2) +
        (y0 + 1) * log(lambda0) - lambda0 * (t1 - t0 + 1/beta)
    )
  }
  
  # function returning the full conditional of lambda1 on log-form
  log.gamma.lambda1 = function(t1, lambda1, beta, y1, t2){
    return(
      (y1+2) * log(t2 - t1 + 1 / beta) - lgamma(y1 + 2) +
      (y1 + 1) * log(lambda1) - lambda1 * (t2 - t1 + 1 / beta)
    )
  }
  
  #calculate acceptance probability
  accept.prob = min(exp(
    log.joint.conditional.t1.lambda0.lambda1(t1.proposal, lambda0.proposal,
                                             lambda1.proposal, y.proposal) -
      log.joint.conditional.t1.lambda0.lambda1(t1.prev, lambda0.prev, lambda1.prev, y.prev) +
      log.gamma.lambda0(t1.prev, lambda0.prev, beta, y.prev[1], t0) -
      log.gamma.lambda0(t1.proposal, lambda0.proposal, beta, y.proposal[1], t0) +
      log.gamma.lambda1(t1.prev, lambda1.prev, beta, y.prev[2], t2) -
      log.gamma.lambda1(t1.proposal, lambda1.proposal, beta, y.proposal[2], t2)
    ), 1)
  
  # get a uniform random number between 0 and 1
  u = runif(1)
  # Return new propsals if u < acceptance probability
  if(u < accept.prob) return(list(t1 = t1.proposal, lambda0 = lambda0.proposal, lambda1 = lambda1.proposal,
                                  y = y.proposal, accepted = 1))
  #else return current values
  else return(list(t1 = t1.prev, lambda0 = lambda0.prev, lambda1 = lambda1.prev, y = y.prev, accepted = 0))
}

```


```{r}
#function used to sample a new beta, lambda0 and lambda1 value using a block proposal.
# First a new beta is proposed from a normal distribution centered at the current value of beta.
# Thereafter lambda0 and lambda1 are generated from their joint full conditionals given
# t1 and proposed beta.
# Lastly the proposals are either accepted or rejected. If rejected, the current values are returned.
# Implemented on log-form for efficiency and avoiding too large or to small values
# sigma.beta is a tuning parameter that determines how large steps sizes are usually taken by beta
# sigma.t1 is a tuning parameter that determines how large step sizes are usually taken by t1
sample.block.beta.normal.walk.and.lambdas = 
  function(beta.prev, lambda0.prev, lambda1.prev, t1, sigma, t0, t2, y, dates) {
  # get proposal for new beta value from normal distribution around current value
  beta.proposal = rnorm(1, mean = beta.prev, sd = sigma)
  # if proposal is outside the allowed range, return the current value of beta, lambda0 and lambda1
  if (beta.proposal <= 0) return(list(beta = beta.prev, lambda0 = lambda0.prev,
                                      lambda1 = lambda1.prev, accepted = 0))

  # get proposal for lambda0 and lambda1 from joint conditional given t1.proposal and beta
  lambda0.proposal = sample.lambda0.full.conditional(t1, beta.proposal, t0, y)
  lambda1.proposal = sample.lambda1.full.conditional(t1, beta.proposal, t2, y)
  

  # function returning the not normalized joint conditional of beta, lambda0 and lambda1 on log-form
  log.joint.conditional.beta.lambda0.lambda1 = function(t1, beta, lambda0, lambda1, y) {
    return(
      - lambda0 * (t1 - t0) - lambda1 * (t2 - t1) - (lambda0 + lambda1 + 1) / beta +
             (y[1] + 1) * log(lambda0) + (y[2] + 1) * log(lambda1) - 5 * log(beta)
      ) 
  }
  
  # function returning the full conditional of lambda0 on log-form
  log.gamma.lambda0 = function(t1, lambda0, beta, y0, t0) {
    return(
      (y0+2) * log(t1 - t0 + 1 / beta) - lgamma(y0 + 2) +
        (y0 + 1) * log(lambda0) - lambda0 * (t1 - t0 + 1/beta)
    )
  }
  
  # function returning the full conditional of lambda1 on log-form
  log.gamma.lambda1 = function(t1, lambda1, beta, y1, t2){
    return(
      (y1+2) * log(t2 - t1 + 1 / beta) - lgamma(y1 + 2) +
      (y1 + 1) * log(lambda1) - lambda1 * (t2 - t1 + 1 / beta)
    )
  }
  
  #calculate acceptance probability
  accept.prob = min(exp(
    log.joint.conditional.beta.lambda0.lambda1(t1, beta.proposal, lambda0.proposal,
                                               lambda1.proposal, y) -
      log.joint.conditional.beta.lambda0.lambda1(t1, beta.prev, lambda0.prev, lambda1.prev, y) +
      log.gamma.lambda0(t1, lambda0.prev, beta.prev, y[1], t0) -
      log.gamma.lambda0(t1, lambda0.proposal, beta.proposal, y[1], t0) +
      log.gamma.lambda1(t1, lambda1.prev, beta.prev, y[2], t2) -
      log.gamma.lambda1(t1, lambda1.proposal, beta.proposal, y[2], t2)
    ), 1)
  #print(accept.prob)
  # get a uniform random number between 0 and 1
  u = runif(1)
  # Return new propsals if u < acceptance probability
  if(u < accept.prob) return(list(beta = beta.proposal, lambda0 = lambda0.proposal,
                                  lambda1 = lambda1.proposal, accepted = 1))
  #else return current values
  else return(list(beta = beta.prev, lambda0 = lambda0.prev, lambda1 = lambda1.prev, accepted = 0))
}
```


```{r}
# Implements a block mcmc formula for approximating the posterior distribution of the
# parameters of interest.
# Using two different block updates
block.mcmc = function(n, K, dates, sigma.t1, sigma.beta,
                      t1.initial = runif(1, dates[1], dates[length(dates)]),
                      beta.initial = 1 / rgamma(1, shape = 2, rate = 1),
                      
                      lambda0.initial = 1 / rgamma(1, shape = 3, scale = beta.initial),
                      
                      lambda1.initial = 1 / rgamma(1, shape = 3, scale = beta.initial)
                      ) {
  "
  Input:
    -n: number of useful samples after the burn-in period wanted
    -K: length of the burn-in period
    -dates: the dates of the explosions (+ start and end date)
    -sigma.t1: the standard error of the normally distributed random walk of t1 centered around
    the current value
    -sigma.beta: the standard error of the normally distributed random walk of beta centered around
    the current value
    -t1.initial: the initial value of t1. If not provided it is sampled from the prior
    -beta.initial: the initial value of beta. If not provided it is sampled from the prior
    -lambda0.initial: the initial value of lambda0. If not provided it is sampled from the
    prior given beta.initial
    -lambda1.initial: the initial value of lambda1. If not provided it is sampled from the
    prior given beta.initial
    
  Output:
    -A list containing
      -- t1: a vector of sampled t1 values
      -- beta: a vector of sampled beta values
      -- lambda0: a vector of sampled lambda0 values
      -- lambda1: a vector of sampled lambda1 values
      -- iter: a sequence going from 0 to n + K
      -- df.parameters: a data frame containing all of the vectors above as columns in
      addition to a column with boolean value of TRUE if the observation is a part of the
      burn-in period
      -- df.parameters.long: a data frame with the same info as df.parameters, but in long-format
      -- n: number of useful samples after the burn-in period wanted
      -- K: length of the burn-in period
      --sigma.t1: the standard error of the normally distributed random walk of t1 centered around
      the current value
      --sigma.beta: the standard error of the normally distributed random walk of beta centered around
      the current value
      -- percent.block1.accepted: the percent of proposed block 1 values accepted
      -- percent.block2.accepted: the percent of proposed block 2 values accepted
  "
  K = K + 1 # to make space for initial values
  t0 = dates[1] # extract start date
  t2 = dates[length(dates)] # extract end date
  date.explosions = dates[2:(length(dates) - 1)] # remove start and end date
  # Create vectors to store values
  t1.values = rep(NA, n+K)
  lambda0.values = rep(NA, n+K)
  lambda1.values = rep(NA, n+K)
  beta.values = rep(NA, n+K)
  
  # Add initial values
  t1.values[1] = t1.initial
  lambda0.values[1] = lambda0.initial
  lambda1.values[1] = lambda1.initial
  beta.values[1] = beta.initial
  
  # create y containing y0 and y1
  y = c(sum(date.explosions <= t1.initial), sum(date.explosions > t1.initial)) 
  
  n.accepted.block1 = 0 # keep track of number of accepted block proposals
  n.accepted.block2 = 0 # keep track of number of accepted t1 proposals (if applicable)
  for (i in 2:(n+K)) {
    if (i %% 2 == 0) {
      # simulate new t1, lambda0 and lambda1 value from first block sample.
      # Get corresponding new y value
      new.block.variables = 
        sample.block.t1.normal.walk.and.lambdas(t1.values[i-1], lambda0.values[i-1],
                                                lambda1.values[i-1], beta.values[i-1],
                                                sigma.t1, t0, t2, y, dates)
  
      t1.values[i] = new.block.variables$t1
      lambda0.values[i] = new.block.variables$lambda0
      lambda1.values[i] = new.block.variables$lambda1
      
      y = new.block.variables$y
      n.accepted.block1 = n.accepted.block1 + new.block.variables$accepted
      
      # The value of beta is unchanged from laster iteration
      beta.values[i] = beta.values[i-1]
    }
    
    if (i %% 2 == 1) {
      # simulate new beta, lambda0 and lambda1 value. Get corresponding new y value
      new.block.variables = sample.block.beta.normal.walk.and.lambdas(beta.values[i-1],
                                                                      lambda0.values[i-1],
                                                                      lambda1.values[i-1],
                                                                      t1.values[i-1], 
                                                                      sigma.beta,
                                                                      t0, t2, y, dates)
        
      beta.values[i] = new.block.variables$beta
      lambda0.values[i] = new.block.variables$lambda0
      lambda1.values[i] = new.block.variables$lambda1
      
      
      # keep track of number of accepted block proposals
      n.accepted.block2 = n.accepted.block2 + new.block.variables$accepted
      
      # the value of t1 is unchanged from last iteration
      t1.values[i] = t1.values[i-1]
    }
  }
  
  # Create a vector with the iteration numbers. The initial values correspond to iteration 0.
  iter = c(seq(0, n+K-1))
  
  # Add parameter values and other interesting information to a data frame
  df.parameters = data.frame(iter = iter, t1 = t1.values, beta = beta.values,
                             lambda0 = lambda0.values, lambda1 = lambda1.values,
                             burn.in = c(rep(TRUE, K), rep(FALSE, n)))
  
  # make the same dataframe on long format
  df.parameters.long = pivot_longer(df.parameters, c(-iter, -burn.in),
                                    names_to = "parameters", values_to = "value")
  
  # return everything of interest in a list for easy access
  return(list(t1 = t1.values, beta = beta.values, lambda0 = lambda0.values, lambda1 = lambda1.values,
              df.parameters = df.parameters, df.parameters.long = df.parameters.long,
              n = n, K = K-1, sigma = sigma.beta, sigma.t1, iter = iter, 
              percent.block1.accepted = n.accepted.block1 / ceiling((n + K - 1) / 2),
              percent.block2.accepted = n.accepted.block2 / floor((n + K - 1)/2) ))
}
```


## 8)

In this task we will run the new algorithm for different values of the tuning parameters and evaluate the burn-in and mixing properties of the algorithm. Additionally we will compare the burn-in and mixing properties of the single site and block Metropolis–Hastings algorithms. Lastly we estimate the marginal posterior distributions.


We begin by plotting trace plots for multiple combinations of tuning parameters.
```{r tuning_alg}
# Decide for which tuning parameter combinations the algorithm will be tested
sigma.values.t1.low = c(1,3,5)
sigma.values.beta.low = c(0.1,0.5,1)
sigma.values.t1.high = c(10,20,50)
sigma.values.beta.high = c(2,2,3)
# set number of iterations performed
n = 20000
K = 0
# set the different seeds for the initial values.
# Chosen such that t_1 begins with a low, medium and high value
seeds = c(12,4,7)


# A function that runs the MCMC block site algorithm and plots.
# Returns a string to be used as figure caption
plot.for.mutiple.tuning.parameters = function(n, K, sigma.values.t1, sigma.values.beta, fig.label) {
  for (i in 1:length(sigma.values.t1)) {
    # set seeds and run for three different initial conditions
    set.seed(seeds[1])
    block.mc.test.tuning.1 = block.mcmc(n,K, df.coal$date, sigma.values.t1[i], sigma.values.beta[i])
    set.seed(seeds[2])
    block.mc.test.tuning.2 = block.mcmc(n,K, df.coal$date, sigma.values.t1[i], sigma.values.beta[i])
    set.seed(seeds[3])
    block.mc.test.tuning.3 = block.mcmc(n,K, df.coal$date, sigma.values.t1[i], sigma.values.beta[i])
    # Combine the realizations to a block data frame for plotting
    df.parameters.test.tuning = rbind(block.mc.test.tuning.1$df.parameters.long,
                                    block.mc.test.tuning.2$df.parameters.long,
                                    block.mc.test.tuning.3$df.parameters.long)
    
    # add a column that keeps track of which realizaiton the observations belong to
    df.parameters.test.tuning$realization = rep(c("1","2","3"),
                                                each = nrow(block.mc.test.tuning.1$df.parameters.long))
    # create the plot
    p = ggplot(df.parameters.test.tuning, aes(x = iter, y = value, col = realization)) +
      
      geom_line(alpha = 0.75) + facet_wrap(~parameters, scale = "free_y", ncol = 4) +
      
      theme(legend.position="bottom") + guides(col = guide_legend("Realizations")) +
      
      xlab("Iteration") + ylab("Value") +
  
      ggtitle(TeX(paste("Trace plot of the parameters for three differently chosen",
                        
        "initial conditions, $\\sigma_{t_1}$ =", sigma.values.t1[i],
        
        "and $\\sigma_{\\beta}$ =", sigma.values.beta[i])),
              
        subtitle = TeX(paste("Average acceptance probability for block 1 and block 2 is respectively",
                                   
       round((block.mc.test.tuning.1$percent.block1.accepted +
                block.mc.test.tuning.2$percent.block1.accepted +
                block.mc.test.tuning.3$percent.block1.accepted) / 3,
              3),
       "and",
     round((block.mc.test.tuning.1$percent.block2.accepted +
                block.mc.test.tuning.2$percent.block2.accepted +
                block.mc.test.tuning.3$percent.block2.accepted) / 3,
              3)))) +
      scale_color_manual(values = c("black", "lightblue", "lightgrey"), labels = c("1", "2", "3"))
    # print the plots
    print(p)
  }
  
  # Create a string with the combinations of tuning parameters as a list
  str.lists = ""
  for (i in 1:length(sigma.values.t1)) {
    str.lists = paste(str.lists, "\\{", sigma.values.t1[i], ", ",
                      sigma.values.beta[i], "\\}", sep = "")
    if ( i < length(sigma.values.t1)) str.lists = paste(str.lists, ", ", sep = "")
  }
  
  # create figure caption. Written in a specific format to be able to show everythin in rmarkdown
fig.caption.tuning = paste("\\label{", fig.label, "}Trace plot of the parameters for tuning ",

    "parameter $\\{ \\sigma_{t_1}, \\sigma_{\\beta} \\} \\in$ [", str.lists, "] and three different ",
    
    "initial conditions. The block site MCMC algorithm was run for ", n, " iterations. ",
    
    "The initial values for realization 1 were; $t_1^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.1$t1[1],2), ", $\\beta^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.1$beta[1],2) , ", $\\lambda_0^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.1$lambda0[1],2), ", $\\lambda_1^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.1$lambda1[1],2), ". ",

    "The initial values for realization 2 were; $t_1^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.2$t1[1],2), ", $\\beta^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.2$beta[1],2) , ", $\\lambda_0^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.2$lambda0[1],2), ", $\\lambda_1^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.2$lambda1[1],2), ". ",

    "The initial values for realization 3 were; $t_1^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.3$t1[1],2), ", $\\beta^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.3$beta[1],2) , ", $\\lambda_0^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.3$lambda0[1],2), ", $\\lambda_1^{\\textrm{initial}} =$ ",
    
    round(block.mc.test.tuning.3$lambda1[1],2), ".", sep = "")
  # return figure caption
  return(fig.caption.tuning)
}
```


```{r, fig.width = 10, fig.height=3, fig.cap = fig_cap_low, eval.after = "fig.cap", fig.show = "hold", out.width = "100%", dependson= "tuning_alg"}
fig_cap_low = plot.for.mutiple.tuning.parameters(n, K, sigma.values.t1.low, sigma.values.beta.low, "fig:block_tuning_low")
```

```{r, fig.width = 10, fig.height=3, fig.cap = fig_cap_high, eval.after = "fig.cap", fig.show = "hold", out.width = "100%", dependson="tuning_alg"}
fig_cap_high = plot.for.mutiple.tuning.parameters(n, K , sigma.values.t1.high, sigma.values.beta.high, "fig:block_tuning_high")
```

In Figure \ref{fig:block_tuning_low} and \ref{fig:block_tuning_high} we plot three realizations from different initial conditions for six combinations of tuning parameters. We do this to evaluate how the tuning parameters affect the burn-in period and mixing properties of the block Metropolis-Hastings algorithm implemented. Similar to the single site algorithm we have tuning parameters related to the expected step length of random walks. While for our single site algorithm the only tuning parameter, $d$, was for the uniform random walk of $t_1$, we have for the block algorithm two tuning parameters $\sigma_{t_1}$ and $\sigma_{\beta}$. These two are respectively the tuning parameter of the normally distributed random walk of $t_1$ and the normally distributed random walk of $\beta$. Equivalent to what we observed for the single site case, we observe that increasing the tuning parameters related to the random walks monotonically decreases the expected burn-in time. For instance for $\sigma_{t_1} = 1$ and $\sigma_{\beta} = 0.1$ we observe from Figure \ref{fig:block_tuning_low} that the burn-in time is around 12000 iterations for one of the realizations, while for $\sigma_{t_1} = 3$ and $\sigma_{t_1} = 0.5$ the burn-in period appear to be completed by iteration 3000 for all three realizations. For $\sigma_{t_1} = 10$ and $\sigma_{\beta} = 2$ the convergence appear to be almost instantaneous. Although convergence is reached faster for higher $\sigma_{t_1}$ and $\sigma_{\beta}$ values we again see that there is a trade-off against acceptance probability. By inspection of Figure \ref{fig:block_tuning_low} and \ref{fig:block_tuning_high} we believe that $\sigma_{t_1} = 8$ and $\sigma_{\beta} = 3$ appear to be a good compromise between fast convergence and high acceptance probability. We will therefore proceed to perform some more diagnostics on the algorithm with this combination of tuning parameteres.

```{r run_mcmc_block}
# run the mcmc algorithm for the new chosen tuning parameters
n = 20000
K = 1000
sigma.beta = 3
sigma.t1 = 8
set.seed(1)
block.mcmc.result = block.mcmc(n, K, df.coal$date, sigma.t1, sigma.beta)
```

Having decided on a pair of tuning parameters, we first check that the chosen burn-in of 1000 iterations is enough. We do this by looking at the trace plot in Figure \ref{fig:trace_plot_block}. From the trace plot it appears that the length of the burn-in period is more than sufficient, with almost instantaneous convergence.

```{r fig_captions8, echo = FALSE}
# Creating the figure caption
fig.caption.trace.plot.block = paste("\\label{fig:trace_plot_block} Trace plot of the parameters $\\beta, \\lambda_0, \\lambda_1$ and $t_1$ for a burn-in period of 1000 iterations and then 20000 iterations of a hopefully converged block MCMC algorithm. The initial values was drawn from the priors and was for this realization; $t_1^{\\textrm{initial}} =$ ", round(block.mcmc.result$t1[1],3), ", $\\beta^{\\textrm{initial}} =$ ", round(block.mcmc.result$beta[1],3), ", $\\lambda_0^{\\textrm{initial}} =$ ", round(block.mcmc.result$lambda0[1],3), ", $\\lambda_1^{\\textrm{initial}} =$ ", round(block.mcmc.result$lambda1[1],3), ". The tuning parameter is $\\sigma_{t_1} = 8$ and $\\sigma_{\\beta} = 3$.", sep = "")

fig.caption.autocorrelation.plot.block = paste("\\label{fig:autocorrelation_plot_block}Autocorrelation plot of the parameters $\\beta, \\lambda_0, \\lambda_1$ and $t_1$ for 20000 iterations after discarding the 1000 burn-in samples. The initial values was drawn from the priors and was for this realization; $t_1^{\\textrm{initial}} =$ ", round(block.mcmc.result$t1[1],3), ", $\\beta^{\\textrm{initial}} =$ ", round(block.mcmc.result$beta[1],3), ", $\\lambda_0^{\\textrm{initial}} =$ ", round(block.mcmc.result$lambda0[1],3), ", $\\lambda_1^{\\textrm{initial}} =$ ", round(block.mcmc.result$lambda1[1],3), ". The tuning parameter is $\\sigma_{t_1} = 8$ and $\\sigma_{\\beta} = 3$. ", sep = "")
```

```{r, fig.width = 10, fig.height = 4, eval.after = "fig.cap", fig.cap = fig.caption.trace.plot.block, dependson=c("fig_captions8", "run_mcmc_block")}
# Plot trace plots of each parameter for a single realization of the block MCMC
ggplot(block.mcmc.result$df.parameters.long, aes(x = iter, y = value)) + geom_line() +
  
  facet_wrap(~parameters, scale = "free_y") +
  
  geom_vline(aes(xintercept = block.mcmc.result$K, col = "Cut off point burn in period"), size = 1) +
  
  theme(legend.position="bottom") + guides(col = guide_legend("Line")) +
  
  xlab("Iteration") + ylab("Value") +

  ggtitle("Trace plot of the parameters",
          
          subtitle = paste("Average acceptance probability for block 1 and block 2 is respectively",
                           round(block.mcmc.result$percent.block1.accepted,3),
                           "and",
                           round(block.mcmc.result$percent.block2.accepted, 3)))
```



```{r, fig.width=8, fig.height=4, fig.show = "hold", out.width="50%", fig.cap = fig.caption.autocorrelation.plot.block, dependson=c("fig_captions8", "run_mcmc_block")}
df.parameters.after.burn.in.block = subset(block.mcmc.result$df.parameters, burn.in == FALSE)
# Plotting the autocorrelation functions for the parameter samples
acf(df.parameters.after.burn.in.block$t1, main = TeX("Autocorrelation for $t_1$ samples"))
acf(df.parameters.after.burn.in.block$beta, main = TeX("Autocorrelation for $\\beta$ samples"))
acf(df.parameters.after.burn.in.block$lambda0, main = TeX("Autocorrelation for $\\lambda_0$ samples"))
acf(df.parameters.after.burn.in.block$lambda1, main = TeX("Autocorrelation for $\\lambda_1$ samples"))
```


```{r}
# Finding estimates for effective sample size
effective.size.block.mcmc = effectiveSize(as.mcmc(df.parameters.after.burn.in.block[,
c("t1", "beta", "lambda0", "lambda1")]))
effective.size.block.mcmc
```


Having established that we have reached convergence by the end of the burn-in period, we analyze the autocorrelation of the samples from the posterior. In Figure \ref{fig:autocorrelation_plot_block} the autocorrelation for different lags are plotted for each of the parameters of interest. We observe that for lag larger than 20 there is little correlation between $t_1$ samples and for lag larger than 25 there is little correlation for $\beta$ samples. For the $\lambda_0$ and $\lambda_1$ samples we observe that the autocorrelation is quite small for lag 15 and higher. Although these are decent results we note that compared to the single site algorithm with tuning parameter $d = 10$ evaluated in A5  the autocorrelation decreases a lot slower for the block algorithm. This is especially obvious for $\beta$ samples where we in A5 observed hardly any autocorrelation at all already for lag 1. A large reason for the significantly slower decrease in autocorrelation for $\beta$, and to a degree $\lambda_0$ and $\lambda_1$ also, might be that $\beta$ is no longer updated from the full conditional, but instead through a random normal walk. When updated through the full conditional a new $\beta$-value is almost independent of the previous $\beta$-value (given $t_1, \lambda_0$ and $\lambda_1$ the update is independent of the previous value, but the value of  $t_1, \lambda_0$ and $\lambda_1$ is dependent on the previous $\beta$-value). Additionally, when updated through the full conditional the acceptance probability is exactly 1, while for the block update with these tuning parameters the acceptance probability was only $`r round(block.mcmc.result$percent.block1.accepted, 3)`$, resulting in the $\beta$-value remaining the same for multiple iterations happening much more often. Both of these contribute greatly to the slower decrease in autocorrelation. 
The fact that $t_1$ is only updated every other iteration might also partially explain why the decrease in autocorrelation for $t_1$ is more than twice as slow as for the single site algorithm in task A5. This is also true for the $\beta$-updates and is another contributing factor to slower decrease in autocorrelation. 

Although algorithm comparison is made difficult due to different tuning parameters and the single site algorithm utilizing uniform random walk while the block algorithm utilizes two normal random walks, it appears that the single site algorithm implemented in task A4 is more efficient. This is also indicated by the estimated effective sample sizes. The estimated effective sample sizes for $t_1, \beta, \lambda_0$ and $\lambda_1$ for the block algorithm with tuning parameters $\sigma_{t_1} = 8$ and $\sigma_{\beta} = 3$ is respectively $[`r toString(round(effective.size.block.mcmc, 2))`]$. Compared to the corresponding estimated effective sample size for the single site algorithm; $[`r toString(round(effective.size.single.site.mcmc, 2))`]$, it is clear that for the same number of iterations we get a lot more effective samples from the single site algorithm.

Comparing the burn-in of the single site and block algorithms is diffult due to the same reasons as mentioned above. As the burn-in period is heavily dependent upon the choice of tuning parameters we can get the burn-in period almost arbitarily small for both algorithms when applied to this problem. Combined with the fact that they utilizes different types of random walks and the tuning parameters are therefore not that easily compared, we struggle to determine with any certainty which algorithm has the best burn-in properties. We do, however, conclude that they both appear to have really fast convergence for this problem if the tuning parameters are chosen appropriately.

Next we want to estimate the marginal posterior distributions. As we concluded that the single site algorithm appeared more efficent, we will use this algorithm to estimate the marginal posteriors.

```{r long_run_single_site}
# run the single site algorithm for many iterations
n = 1000000
K = 10000
d = 10
set.seed(1)
single.mc.result.long.run = single.mcmc(n, K , df.coal$date, d)
```


```{r, fig.width = 10, fig.height = 4, fig.cap = fig.caption.histogram.plot.long.run, dependson=c("long_run_single_site", "fig_cap_long_run_hist"), fig.cap = "\\label{fig:hist_marginals}Estimates of the marginal posterior distributions based on 1000000 samples from the single site MCMC algorithm with tuning parameter $d = 10$."}
# Estimating the mean of the marginal posteriors
mean.t1 = mean(single.mc.result.long.run$t1)
mean.beta = mean(single.mc.result.long.run$beta)
mean.lambda0 = mean(single.mc.result.long.run$lambda0)
mean.lambda1 = mean(single.mc.result.long.run$lambda1)

cov.lambda0.lambda1 = cov(single.mc.result.long.run$lambda0,
                          single.mc.result.long.run$lambda1)

df.means = data.frame(Mean = c(mean.t1, mean.beta, mean.lambda0, mean.lambda1),
                      parameters = c("t1", "beta", "lambda0", "lambda1"))

# Plot histograms of each parameter for a single realization of the single site MCMC
ggplot(subset(single.mc.result.long.run$df.parameters.long, burn.in == FALSE),
       aes(x = value, y = ..density..)) + geom_histogram(colour = "lightgrey", bins = 100) +
  geom_vline(data = df.means, aes(xintercept = Mean, col = "Mean of the posterior marginal")) + theme(legend.position = "bottom") + guides(col = guide_legend("Vertical lines")) +
  facet_wrap(~parameters, scale = "free") + ggtitle("Estimates of posterior marginals")
```

In Figure \ref{fig:hist_marginals} the estimated marginal posterior distributions $\pi(t_1 | \vect x), \pi(\beta | \vect x), \pi(\lambda_0 | \vect x)$ and $\pi(\lambda_1 | \vect x)$ is plotted. The estimates are based on one million samples from the single site MCMC algorithm. As discussed in task A5 the estimated posterior marginals appear very reasonable. We also estimate the mean of $t_1, \lambda_0$ and $\lambda_1$ along with the covariance between $\lambda_0$ and $\lambda_1$.

\begin{align*}
  \E [t_1 | \vect x] &= `r mean.t1` \\
  \E [\lambda_0 | \vect x] &= `r mean.lambda0` \\
  \E [\lambda_1 | \vect x] &= `r mean.lambda1` \\
  \Cov [\lambda_0, \lambda_1] &= `r cov.lambda0.lambda1`.
\end{align*}

We note that $\lambda_0 | \vect x$ and $\lambda_1 | \vect x$ appear to be practically independent of each other.


# Problem B:

In this problem we are going to estimate the posterior marginals of the smooth effect of time series data. First a block Gibbs sampler is implemented and applied to the data set and then we implement an INLA-scheme to solve the same problem for comparison. Furthermore we apply the R-INLA on the dataset and finally compare the results from all three methods. 


First we have to load the dataset from github:

```{r}
library(ggplot2)

gauss_data_csv <- read.csv("https://raw.githubusercontent.com/ArneRustad/TMA4300_Computiational_Statistics/main/Gaussian%20Data.csv", header=FALSE)

gauss_data = data.frame(y = gauss_data_csv[,1], t = seq(1,length(gauss_data_csv[,1])))
```

```{r, fig.cap="Scatter plot of the data points from the gaussian distributed data."}
ggplot(gauss_data, aes(x=t, y=y) ) +
  geom_point()
```

By inspecting the plot it is reasonable to assume that the data is distributed around some mean which looks like a third or fourth degree polynomial.


We assume that, given the vector $\vect \eta =(\eta_1, \dots, \eta_T)$ the observations $y_t$ are independent and Gaussian distributed with mean $\eta_t$ and known unit variance:

\begin{align*}
y_t | \eta_t &\sim \mathcal{N} (\eta_t, 1); \qquad t = 1, \dots, T \\
\pi(y | \eta_t) &= \frac{1}{\sqrt{2\pi}} e^{ \frac{1}{2}(y_t - \eta_t)^2} 
\end{align*}
The linear predictor $\eta_t$ is linked to a smooth effect of time as $\eta_t = f_t$ and we choose to use a second order random walk model as prior distribution for the vector $\vect f = (f_1, \dots, f_T)$, so that

\begin{equation} \label{eq:prior_f}
\pi(\vect f | \theta) \propto \theta^{T-2} 
\exp \left \{-\frac{\theta}{2} \sum_{t=3}^T (f_t - 2 f_{t-1} + f_{t-2})^2 \right \} 
\sim \mathcal{N} (0, \matr Q(\theta)
\end{equation}

Thus, $\vect f | \theta)$ is Gaussian distributed with mean $\vect 0$ and precision matrix $\matr Q(\theta)$. The precicion parameter $\theta$ controls the smoothness of the vector $\vect f$ and we assign $\theta$ a $\textrm{Gamma}(1,1)$ distribution as prior.

Our main inferential interest lies in the posterior marginal for the smooth effect $\pi(\eta_t|\vect{y}), t = 1, . . . , T.$.

## 1)

We want to use a latent Gaussian model to estimate the smooth effect of this time series data. A latent Gaussian model (LGM) consist of three elements: a likelihood model, a latent Gaussian field and a vector of hyperparameters. In this particular model we have a single hyperparameter $\vect{\theta}$ which controls the smoothness of the temporal effect $\vect{f} = (f_1, ..., f_T)$. We have a latent Gaussian field $\vect{x} \equiv \{ \vect \eta, \vect{f}\}$ and we have a likelihood $\pi(\vect{y}|\vect{\eta}, \theta) = \Pi_{t=1}^{T}\pi(y_t|\eta_t, \theta)$. 

The observed data $\vect{y}$ is assumed to be conditionally independent given the latent Gaussian field $\vect{x} = \{ \vect{\eta}, \vect{f}\}$ so that the univariate model for the likelihood describes the  marginal distribution of the observation. In a generalized linear model (GLM) framework of an obersvation $y_i$, the observation is linked to a Gaussian linear predictor $\eta_i$ through a link function. In this exercise the link function is the identity link function. The linear predictor $\eta_t$ is linked to a smooth effect of time $f_t$ as $\eta_t = f_t$, where $\vect f$ is a second order random walk which simulates the temporal effects. Considering the elements from the LGM the model can formally be written as:

\begin{align*}
\vect{y}|\vect{\eta}, \theta &\sim \Pi_{i=1}^{T}\pi(y_i|\eta_i, \theta)\\
\vect{\eta}|\theta &\sim \mathcal{N}(0, (\vect{Q}(\theta)^{-1}))\\
\theta &\sim \pi(\theta)
\end{align*}


where $\vect{Q}(\theta)$ is the precision matrix of the latent Gaussian field and it is generally assumed to be sparse. In our case it is a band matrix with bandwith 2. 

We will now derive an expression for the precision matrix $\vect{Q}(\theta)$ where we use the definition in Equation (\ref{eq:prior_f}).

\begin{equation*}
\frac{\theta}{2} \sum_{t=3}^T [f_t - 2f_{t-1} + f_{t-2}]^2 = -\frac{1}{2}\vect{f}^T\vect{Q}(\theta)\vect{f} = -\frac{1}{2}\vect{f}^T\vect{Q}_0\theta\vect{f}
\end{equation*}

where $\matr{Q}_0$ is the precision matrix independent of the hyperparameter $\theta$. The factorization of $\matr{Q}_0$ and $\theta$ done in the above equation is evident from the form of the expression of the left hand side of the equation. By comparing the sum and the matrix form we get that $\sum_{t=3}^T [f_t - 2f_{t-1} + f_{t-2}]^2 = \mathbf{f}^T\mathbf{Q}_0\mathbf{f}$. There are several ways of deriving the expression for the precision matrix, but here we will do it by comparing the elements of the sum:

\begin{align*}
[f_t - 2f_{t-1} + f_{t-2}]^2 &= f_t^2 + (2f_{t-1})^2 + f_{t-2}^2 + f_t(- 2f_{t-1} + f_{t-2}) - 2f_{t-1}(f_t + f_{t-2}) + f_{t-2}(f_t - 2f_{t-1})\\
&=  f_t^2 + 4f_{t-1}^2 + f_{t-2}^2 + f_t(- 2f_{t-1} + f_{t-2}) - 2f_{t-1}(f_t + f_{t-2}) + f_{t-2}(f_t - 2f_{t-1})
\end{align*}

It follows that in matrix format this is

\begin{equation*}
\mathbf{f}^T\mathbf{Q}_0\mathbf{f} = \begin{bmatrix}
f_1 & \hdots & f_T
\end{bmatrix}
\begin{vmatrix}
1 & -2 & 1 \\
-2 & 5 & -2 & 1 &&\Huge0\\
1 & -2 & 6 & -2 & 1 \\
 & \ddots & \ddots & \ddots & \ddots & \ddots \\
 &&1 &-2 & 6 & -2 & 1\\
 \Huge0&&& 1& -2 & 5 & -2\\
 &&&&1 & -2 & 1 
\end{vmatrix}
\begin{bmatrix}
f_1 \\
\vdots\\
f_T
\end{bmatrix}
.
\end{equation*}

and then we have the expression for the $\theta$ dependent precision matrix $\mathbf{Q}(\theta)$. The algorithm which generates this matrix is implemented below.


```{r}
gen_Q = function(n_samples){
  Q = matrix(0, nrow = n_samples, ncol = n_samples)
   for(i in 1:n_samples){
     for(j in 1:n_samples){
       if(i == j){
         if(i == 1 || i == n_samples){Q[i,j] = 1}
         else if(i == 2 || i == n_samples - 1){Q[i,j]= 5}
         else Q[i,j] = 6
       }
       if( i == j + 1 || i == j-1){
          if(i + j == 3 || i + j == 2*n_samples - 1){Q[i,j] = -2}
          else Q[i,j] = -4
       }
       if(i == j - 2 || i == j+2){Q[i,j] = 1}
     }
   }
  return(Q)
}

pi_theta_given_etay = function(Q_matrix, y, eta){
  ym = as.matrix(y)
  etam = as.matrix(eta)
  Q_m = as.matrix(Q_matrix)
  distr = (2*pi)^(-length(ym))*det(Q_m)*exp(-(1/2)*t(ym - etam)%*%(Q_m + diag(length(Q_m[,1])))%*%(ym - etam) - (1/2)*t(ym)%*%Q_m%*%etam + t(etam)%*%Q_m%*%ym + (1/2)*t(etam)%*%Q_m%*%etam)
  return(distr)
}

#Generating the precision matrix from the random walk sum
Q_nbn = gen_Q(n_samples = length(gauss_data[,1]))
#print to confirm correct form
#print(Q_nbn)

#trying to generate sigma
sigma = solve(Q_nbn + diag(length(gauss_data[,1])))

eta0 = gauss_data + runif(n= length(gauss_data), min = 0.1, max = 0.2)
test = pi_theta_given_etay(Q_matrix = Q_nbn, y = gauss_data, eta = eta0)

```

Integrated Nested Laplace Approximations is a way to do inference on latent Gaussian models (LGM) under the following assumptions:

1. Each data point depends only on one of the elements in the latent Gaussian field, $\mathbf{x}$, which is the linear predictor such that the likelihood can be written as:

\begin{equation*}
\mathbf{y}|\mathbf{x}, \vect \theta \sim \prod_i \pi (y_i, \eta_i, \vect{\theta}). 
\end{equation*} 

2. The size of the hyperparameter vector $\theta$ is small, for example $\theta<15$.

3. The latent field $\mathbf{x}$, can be large but it is endowed with some conditional independence (Markov) properties so that the precision matrix $Q(\vect{\theta})$ is sparse.

4. The linear predictor depends linearly on the unknown smooth function of covariates.

5. The inferential interest lies in the univariate posterior marginals $\pi(x_i|y)$ and $\pi(\theta_j|y)$ rather than in the joint posterior $\pi(x, \theta|y)$.

Since we have assumed each observation $y_i$ to be independently normally distributed around $\eta_t$ with unit variance we obviously have that each data point depends only on one of the elements in the latent Gaussian field. Since we only have one hyperparameter $\theta$, assumption 2 is also satisfied. We have also shown that the precision matrix $Q(\mathbf{\theta})$ is sparse thereby satisfying assumption 3. As we have assumed $\eta_t = f_t$, the linear predictor depends linearly on the unknown smooth function of covariates, proving that assumption 4 is satisfied. Lastly, as we are interested in univariate posterior marginals, we have that assumption 5 is satisfied. Since our chosen model is a LGM we have thereby shown that all the neccecary assumptions to be able to use INLA inference is satisfied.





## 2)

Now that we have the formal expression of the latent Gaussian we can continue with defining the block Gibbs sampling algorithm that we will use:

1. Initiate $\eta = \mathbf{0}$ and initiate $\theta$ by sampling $\theta \sim \mathrm{Gamma}(1,1)$.

2. Sample $\theta_{new}$ from the full conditional $\pi (\theta|\mathbf{\eta}, \mathbf{y})$.

3. Sample $\eta_{new}$ from the full conditional $\pi (\eta|\mathbf{\theta}, \mathbf{y})$.

4. Repeat step 2 and 3 until convergence.

5. Run step 2 and 3 until you have a desired amount of samples.

In the block Gibbs sampling algorithm all samples are accepted.

#### Calculating the full conditional $\pi (\theta|\mathbf{\eta}, \mathbf{y})$
We find an expression for the full conditional of $\theta$ by using that it is proportional to the joint posterior distribution $\pi(\mathbf{\eta}, \theta|\mathbf{y})$:


\begin{equation*}
\pi(\theta|\eta, \mathbf{y}) \propto \pi(\mathbf{\eta}, \theta|\mathbf{y}) \\
\pi(\theta|\eta, \mathbf{y}) \propto \pi(\mathbf{y}|\mathbf{\eta}) \pi(\mathbf{\eta}|\theta)\pi(\theta) \\
\pi(\theta|\eta, \mathbf{y}) \propto \pi(\mathbf{y}|\mathbf{\eta}) \pi(\mathbf{f}|\theta)\pi(\theta)\\
\end{equation*}







